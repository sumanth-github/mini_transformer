{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumanth-github/mini_transformer/blob/main/llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o7vLo55YUGI-"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb\n",
        "!pip install tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ahwOq6B3bct4"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TQWa9lKLdfQ",
        "outputId": "cd5bed32-e692-4c85-b793-05e0235f468c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2x2czTq2vshQKZ0WK5yTPHJM7Ml_2KSCazfnhEDPtWrnuKHom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "06NA91r6bf4I",
        "outputId": "df80cde5-61cc-4cd5-f676-378363b0f977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.47.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.47.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.47.1 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jhR7n9W95Tj",
        "outputId": "ca40af52-5abc-446d-855a-e9b668dec895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your app is live at: NgrokTunnel: \"https://8531403103f4.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import streamlit as st\n",
        "\n",
        "# Write your app.py content\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write('''\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"Tiny LLM Chatbot\")\n",
        "user_input = st.text_input(\"You: \")\n",
        "\n",
        "if user_input:\n",
        "    # Dummy response logic, replace with real model\n",
        "    st.write(\"Bot:\", \"This is a placeholder response.\")\n",
        "''')\n",
        "\n",
        "# Start the Streamlit app\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "\n",
        "# Expose it with ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Your app is live at:\", public_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "# config.py\n",
        "MAX_LEN    = 128\n",
        "D_MODEL    = 64\n",
        "NUM_HEADS  = 4\n",
        "D_FF       = 128\n",
        "NUM_LAYERS = 3\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 100\n",
        "LR         = 5e-4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj2LZqy6QwG5",
        "outputId": "caa1a8e7-209c-43de-dd11-aa15a71b3961"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpx22dXU95N7",
        "outputId": "0f782085-2282-47b4-9bbe-3eac4fc1c9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Positional Encoding Layer\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1), :].to(x.device)\n",
        "\n",
        "\n",
        "# One Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        ff_out = self.ff(x)\n",
        "        return self.norm2(x + self.dropout(ff_out))\n",
        "\n",
        "# Full Transformer Encoder (stack of layers)\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size + 1, d_model, padding_idx=0)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x + self.pos_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# TinyQA Model: Encoder + Decoder (CLS-style output)\n",
        "class TinyQAModel(nn.Module):\n",
        "    def __init__(self, encoder, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = nn.Linear(d_model, vocab_size)  # Project [CLS] token output to vocab\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)           # shape: (batch, seq_len, d_model)\n",
        "        return self.decoder(encoded)      # output: (batch, vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2ZHT_bB95Lh",
        "outputId": "81be53a2-460d-4efe-a974-4656d0aafa40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils.py\n",
        "import torch\n",
        "from chromadb import Client\n",
        "import chromadb.utils.embedding_functions as embedding_functions\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Initialize embedder\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "chroma_client = Client()\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"rag_memory\")\n",
        "\n",
        "def embed_chunks(chunks):\n",
        "    embeddings = embedder.encode(chunks, show_progress_bar=True)\n",
        "    ids = [f\"id_{i}\" for i in range(len(chunks))]\n",
        "    chroma_collection.add(documents=chunks, embeddings=embeddings, ids=ids)\n",
        "    return len(chunks)\n",
        "\n",
        "def build_rag_prompt(user_query, top_k=3):\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import chromadb\n",
        "    chroma_client = chromadb.Client()\n",
        "    collection = chroma_client.get_or_create_collection(\"rag_memory\")\n",
        "\n",
        "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    query_embedding = embedder.encode([user_query])[0]\n",
        "\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    context_chunks = results[\"documents\"][0]\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "\n",
        "    return f\"Context:\\n{context}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
        "\n",
        "\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1, \"<EOS>\": 2}\n",
        "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\", 2: \"<EOS>\"}\n",
        "        self.vocab_size = 3\n",
        "\n",
        "    def fit(self, texts):\n",
        "        for text in texts:\n",
        "            for word in text.strip().split():\n",
        "                if word not in self.word2idx:\n",
        "                    idx = len(self.word2idx)\n",
        "                    self.word2idx[word] = idx\n",
        "                    self.idx2word[idx] = word\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "\n",
        "    def encode(self, text, max_len=None, eos=True):\n",
        "        tokens = [self.word2idx.get(word, 1) for word in text.strip().split()]\n",
        "        if eos:\n",
        "            tokens.append(2)\n",
        "        if max_len:\n",
        "            tokens = tokens[:max_len] + [0] * max(0, max_len - len(tokens))\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return \" \".join([self.idx2word.get(token, \"<UNK>\") for token in tokens if token != 0])\n",
        "\n",
        "    def get_pad_id(self):\n",
        "        return 0\n",
        "\n",
        "    def get_eos_id(self):\n",
        "        return 2\n",
        "\n",
        "def prepare_data(pairs, tokenizer, max_len, pad_id):\n",
        "    inputs, targets = [], []\n",
        "    for q, a in pairs:\n",
        "        q_ids = tokenizer.encode(q)[:max_len//2]\n",
        "        a_ids = tokenizer.encode(a)[:max_len//2]\n",
        "        if hasattr(tokenizer, 'sp'):\n",
        "            eos_id = tokenizer.sp.eos_id()\n",
        "        else:\n",
        "            eos_id = 1  # fallback for SimpleTokenizer (or define a constant)\n",
        "\n",
        "        input_seq = q_ids + [eos_id]\n",
        "        target_seq = a_ids + [eos_id]\n",
        "\n",
        "        # Pad sequences\n",
        "        input_seq += [pad_id] * (max_len - len(input_seq))\n",
        "        target_seq += [pad_id] * (max_len - len(target_seq))\n",
        "\n",
        "        inputs.append(input_seq)\n",
        "        targets.append(target_seq)\n",
        "\n",
        "    X = torch.tensor(inputs, dtype=torch.long)\n",
        "    y = torch.tensor(targets, dtype=torch.long)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BPETokenizer:\n",
        "    def __init__(self, model_file=\"chatbot_bpe.model\"):\n",
        "        import sentencepiece as spm\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.load(model_file)\n",
        "\n",
        "    def encode(self, text, max_len=None):\n",
        "        tokens = self.sp.encode(text, out_type=int)\n",
        "        if max_len:\n",
        "            tokens = tokens[:max_len] + [0] * max(0, max_len - len(tokens))\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        return self.sp.decode(token_ids).replace(\"‚ñÅ\", \" \").strip()\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return self.sp.get_piece_size()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "1zU4e1y3iRqL"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "PmwncOcQ94_b",
        "outputId": "51e92db6-fb7f-4d85-a8ef-a44b16bbf8a4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-da8d7048-c8ca-485d-8846-a1347ebe3f4a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-da8d7048-c8ca-485d-8846-a1347ebe3f4a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving augmented_qa_dataset.json to augmented_qa_dataset.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "ewft3hbk948-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"augmented_qa_dataset.json\", \"r\") as f:\n",
        "    qa_pairs = json.load(f)\n",
        "\n",
        "with open(\"bpe_train.txt\", \"w\") as f_out:\n",
        "    for q, a in qa_pairs:\n",
        "        f_out.write(q.strip() + \"\\n\")\n",
        "        f_out.write(a.strip() + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiHI0WeX946_",
        "outputId": "60f7d21a-9936-4166-b646-79bede311b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=\"bpe_train.txt\",\n",
        "    model_prefix=\"chatbot_bpe\",\n",
        "    vocab_size=800,\n",
        "    model_type=\"bpe\",\n",
        "    character_coverage=1.0,\n",
        "    pad_id=0,\n",
        "    unk_id=1,\n",
        "    bos_id=-1,  # disables BOS\n",
        "    eos_id=2,\n",
        "    hard_vocab_limit=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MKKXd6H95Jc",
        "outputId": "2f4345fe-4133-485e-f031-a3c22c47b1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train.py\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from model import TransformerEncoder, TinyQAModel\n",
        "from utils import BPETokenizer, prepare_data\n",
        "\n",
        "# ==== Load the full dataset ====\n",
        "import json\n",
        "with open(\"augmented_qa_dataset.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(data)} Q&A pairs\")\n",
        "\n",
        "# Add some basic conversational examples\n",
        "conversational_data = [\n",
        "    (\"Hi\", \"Hello!\"),\n",
        "    (\"Hello\", \"Hi there!\"),\n",
        "    (\"Hey\", \"Hey! How can I help you?\"),\n",
        "    (\"How are you?\", \"I'm doing well, thanks for asking!\"),\n",
        "    (\"What's your name?\", \"I'm TinyBot, your AI assistant.\"),\n",
        "    (\"Thank you\", \"You're welcome!\"),\n",
        "    (\"Thanks\", \"Happy to help!\"),\n",
        "    (\"Bye\", \"Goodbye!\"),\n",
        "    (\"Goodbye\", \"See you later!\"),\n",
        "]\n",
        "\n",
        "# Combine datasets\n",
        "data = conversational_data + data  # Use first 100 from dataset to keep training manageable\n",
        "\n",
        "# ==== Hyperparameters ====\n",
        "max_len = 128\n",
        "d_model = 64  # Increased for better capacity\n",
        "num_heads = 4\n",
        "d_ff = 128\n",
        "num_layers = 3  # More layers for better understanding\n",
        "batch_size = 8\n",
        "num_epochs = 100  # More epochs for better learning\n",
        "learning_rate = 5e-4\n",
        "\n",
        "# ==== Tokenizer & Data ====\n",
        "tokenizer = BPETokenizer(\"chatbot_bpe.model\")\n",
        "eos_id = tokenizer.sp.eos_id()\n",
        "pad_id = tokenizer.sp.pad_id()\n",
        "vocab_size=tokenizer.sp.get_piece_size()\n",
        "print(f\"üß† Vocab size: {vocab_size}, PAD: {pad_id}, EOS: {eos_id}\")\n",
        "\n",
        "# Prepare data\n",
        "X, y = prepare_data(data, tokenizer, max_len, pad_id)\n",
        "dataset = TensorDataset(X, y)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# ==== Model ====\n",
        "encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len=max_len)\n",
        "model = TinyQAModel(encoder, d_model, vocab_size)\n",
        "\n",
        "# ==== Training ====\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)  # Ignore padding\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "loss_history = []\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_x, batch_y in loader:\n",
        "        logits = model(batch_x)  # shape: [batch, seq_len, vocab_size]\n",
        "        logits = logits.view(-1, vocab_size)  # [batch * seq_len, vocab_size]\n",
        "        targets = batch_y.view(-1)            # [batch * seq_len]\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(logits, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    loss_history.append(avg_loss)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {avg_loss:.4f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "# Save model and tokenizer\n",
        "torch.save(model.state_dict(), \"tinyqa_model.pth\")\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(\"‚úÖ Training complete. Model saved.\")\n",
        "\n",
        "# Test the model\n",
        "print(\"\\nüß™ Testing the model:\")\n",
        "test_questions = [\"Hi\", \"What's your name?\", \"Tell me a joke\", \"Thanks\"]\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for q in test_questions:\n",
        "        input_ids = tokenizer.encode(q)[:max_len]\n",
        "        input_len=len(input_ids)\n",
        "        generated = input_ids + [pad_id] * (max_len - len(input_ids))\n",
        "\n",
        "        for step in range(20):\n",
        "            input_tensor = torch.tensor([generated[:max_len]], dtype=torch.long)\n",
        "            logits = model(input_tensor)\n",
        "            next_pos = input_len + step\n",
        "            if next_pos >= max_len:\n",
        "                break\n",
        "            token_logits = logits[0, next_pos - 1, :]\n",
        "            next_token_id = torch.argmax(token_logits).item()\n",
        "            if next_token_id in [pad_id, eos_id]:\n",
        "                break\n",
        "            generated[next_pos] = next_token_id\n",
        "\n",
        "        answer_ids = generated[input_len:next_pos]\n",
        "        answer = tokenizer.decode(answer_ids) if answer_ids else \"[No response]\"\n",
        "        print(f\"Q: {q} ‚Üí A: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqVR-K1ojUV6",
        "outputId": "612269cb-22d4-4ae5-88be-5213d3e9b060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-02 15:17:16.234506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754147836.307637  100776 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754147836.330150  100776 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loaded 44 Q&A pairs\n",
            "üß† Vocab size: 800, PAD: 0, EOS: 2\n",
            "Starting training...\n",
            "Epoch 10/100: Loss = 5.1903, LR = 0.000500\n",
            "Epoch 20/100: Loss = 4.4705, LR = 0.000250\n",
            "Epoch 30/100: Loss = 4.1436, LR = 0.000250\n",
            "Epoch 40/100: Loss = 3.8452, LR = 0.000125\n",
            "Epoch 50/100: Loss = 3.6576, LR = 0.000125\n",
            "Epoch 60/100: Loss = 3.4903, LR = 0.000063\n",
            "Epoch 70/100: Loss = 3.4326, LR = 0.000063\n",
            "Epoch 80/100: Loss = 3.3602, LR = 0.000031\n",
            "Epoch 90/100: Loss = 3.3015, LR = 0.000031\n",
            "Epoch 100/100: Loss = 3.2647, LR = 0.000016\n",
            "‚úÖ Training complete. Model saved.\n",
            "\n",
            "üß™ Testing the model:\n",
            "Q: Hi ‚Üí A: Hello Hi Hello Hi Hello Hi Hello Hi Hello Hi Hello Hi Hello Hi Hello Hi Hello Hi Hello\n",
            "Q: What's your name? ‚Üí A: , a is of\n",
            "Q: Tell me a joke ‚Üí A: t a is is is is of 42. is is a is a is of\n",
            "Q: Thanks ‚Üí A: H H H H H H H H H H H H H H\n"
          ]
        }
      ],
      "source": [
        "!python train.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaE5gMrg95G3",
        "outputId": "fc419e8e-342b-4acc-b38b-59b9efe4efb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting inference.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile inference.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "from model import TransformerEncoder, TinyQAModel\n",
        "from utils import BPETokenizer\n",
        "\n",
        "# ==== Load tokenizer & hyperparams ====\n",
        "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "# Must match training parameters\n",
        "max_len = 128\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "d_ff = 128\n",
        "num_layers = 3\n",
        "\n",
        "# ==== Load model ====\n",
        "vocab_size = tokenizer.sp.get_piece_size()\n",
        "encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len=max_len)\n",
        "model = TinyQAModel(encoder, d_model, vocab_size)\n",
        "\n",
        "# Load state dict\n",
        "state_dict = torch.load(\"tinyqa_model.pth\", map_location=torch.device('cpu'))\n",
        "for k, v in state_dict.items():\n",
        "    if 'embedding.weight' in k:\n",
        "        print(f\"üìè From checkpoint: {k} = {v.shape}\")\n",
        "print(f\"üìè Your model: embedding.weight = {model.encoder.embedding.weight.shape}\")\n",
        "\n",
        "state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"encoder.pos_encoding.pe\")}\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "\n",
        "def clean_decode(tokens, tokenizer):\n",
        "    words = tokenizer.decode(tokens).split()\n",
        "    cleaned = []\n",
        "    for w in words:\n",
        "        if w not in (\"<PAD>\", \"<UNK>\", \"NULL\"):\n",
        "            if not cleaned or cleaned[-1] != w:\n",
        "                cleaned.append(w)\n",
        "    return \" \".join(cleaned).strip()\n",
        "\n",
        "def answer_question(model, tokenizer, prompt: str, max_gen_len=50, temperature=0.7, max_len=128):\n",
        "    persona_prompt = \"You are a witty and helpful assistant. Answer briefly.\\n\"\n",
        "    input_text = persona_prompt + prompt\n",
        "    input_ids = tokenizer.encode(input_text)[:max_len]\n",
        "    input_len = len([x for x in input_ids if x != 0])\n",
        "\n",
        "    generated = input_ids.copy()\n",
        "    eos_id = tokenizer.sp.eos_id()\n",
        "    pad_id = tokenizer.sp.pad_id()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step in range(max_gen_len):\n",
        "            input_tensor = torch.tensor([generated[:max_len]], dtype=torch.long)\n",
        "            logits = model(input_tensor)\n",
        "            pos = len(generated) - 1\n",
        "            if pos >= max_len - 1:\n",
        "                break\n",
        "\n",
        "            next_token_logits = logits[0, pos, :] / temperature\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_token_id = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            print(f\"üîé Step {step}: token {next_token_id} ‚Üí '{tokenizer.decode([next_token_id])}'\")\n",
        "\n",
        "            if next_token_id == eos_id or next_token_id == pad_id:\n",
        "                print(\"üö© Decoding stopped (EOS or PAD)\")\n",
        "                break\n",
        "\n",
        "            generated.append(next_token_id)\n",
        "\n",
        "    answer_ids = [token for token in generated[input_len:] if token != pad_id and token != 0]\n",
        "    answer = None\n",
        "\n",
        "    if answer_ids:\n",
        "        raw_answer = tokenizer.decode(answer_ids)\n",
        "        cleaned = clean_decode(answer_ids, tokenizer)\n",
        "\n",
        "        print(f\"ü¶™ Raw decoded: '{raw_answer}'\")\n",
        "        print(f\"ü¶π Cleaned answer: '{cleaned}'\")\n",
        "\n",
        "        if not cleaned.strip():\n",
        "            answer = raw_answer.strip() or \"[empty]\"\n",
        "        elif cleaned.lower() in [\"null\", \"none\", \"pad\", \"<pad>\", \"<unk>\"]:\n",
        "            answer = raw_answer.strip()\n",
        "        else:\n",
        "            answer = cleaned\n",
        "    else:\n",
        "        answer = raw_answer.strip() if 'raw_answer' in locals() else \"[no output]\"\n",
        "\n",
        "    return answer\n",
        "\n",
        "def load_model(model_path=\"tinyqa_model.pth\", tokenizer_path=\"chatbot_bpe.model\", max_len=128,\n",
        "               d_model=64, num_heads=4, d_ff=128, num_layers=3, device=\"cpu\"):\n",
        "    with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "\n",
        "    vocab_size = tokenizer.sp.get_piece_size()\n",
        "    encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len=max_len)\n",
        "    model = TinyQAModel(encoder, d_model, vocab_size)\n",
        "\n",
        "    state_dict = torch.load(model_path, map_location=torch.device(device))\n",
        "    state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"encoder.pos_encoding.pe\")}\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ü§ñ TinyBot is ready! Type 'exit' to quit.\\n\")\n",
        "    test_examples = [\"Hi\", \"What's your name?\", \"Thanks\"]\n",
        "    print(\"Testing with examples:\")\n",
        "    for ex in test_examples:\n",
        "        answer = answer_question(model, tokenizer, ex, temperature=0.5)\n",
        "        print(f\"Q: {ex}\")\n",
        "        print(f\"A: {answer}\\n\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"You: \")\n",
        "        if q.lower() in ['exit', 'quit', 'bye']:\n",
        "            print(\"Bot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        answer = answer_question(model, tokenizer, q, temperature=0.5)\n",
        "        print(f\"Bot: {answer}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXGLsd_Y95En",
        "outputId": "ac604f2f-ed9f-4201-c0c4-8e41abab8d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import json\n",
        "import datetime\n",
        "from inference import answer_question,load_model\n",
        "from utils import BPETokenizer\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = load_model(\n",
        "    model_path=\"tinyqa_model.pth\",\n",
        "    tokenizer_path=\"chatbot_bpe.model\",\n",
        "    max_len=128,\n",
        "    d_model=64,\n",
        "    num_heads=4,\n",
        "    d_ff=128,\n",
        "    num_layers=3,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "# ------------------ Session State ------------------\n",
        "def init_session():\n",
        "    if \"history\" not in st.session_state:\n",
        "        st.session_state.history = []\n",
        "\n",
        "# ------------------ Sidebar ------------------\n",
        "def render_sidebar():\n",
        "    st.sidebar.title(\"üß† Tiny Transformer\")\n",
        "    st.sidebar.markdown(\"Crafted by: **Sumanth Kadarla**\")\n",
        "    st.sidebar.markdown(\"üéì B.Tech CSE (AIML), Tier-3\")\n",
        "    st.sidebar.markdown(\"üìç India | üöÄ FAANG-bound\")\n",
        "    st.sidebar.markdown(\"---\")\n",
        "    return st.sidebar.radio(\"üìÇ Navigate\", [\n",
        "        \"üè† Home\",\n",
        "        \"üí¨ QA Chat\",\n",
        "        \"üìú History\",\n",
        "        \"üìö Dataset\",\n",
        "        \"üìä Training Monitor\",\n",
        "        \"‚öôÔ∏è Model Settings\",\n",
        "        \"üß† Architecture\",\n",
        "        \"üìò About\"\n",
        "    ])\n",
        "    st.sidebar.subheader(\"üìÇ Load Saved Model\")\n",
        "    model_file = st.sidebar.file_uploader(\"Upload Model (.pth)\", type=\"pth\")\n",
        "    tokenizer_file = st.sidebar.file_uploader(\"Upload Tokenizer (.model)\", type=\"model\")\n",
        "\n",
        "    if model_file and tokenizer_file:\n",
        "        with open(\"uploaded_model.pth\", \"wb\") as f:\n",
        "            f.write(model_file.read())\n",
        "        with open(\"uploaded_tokenizer.model\", \"wb\") as f:\n",
        "            f.write(tokenizer_file.read())\n",
        "\n",
        "        st.session_state.custom_model = \"uploaded_model.pth\"\n",
        "        st.session_state.custom_tokenizer = \"uploaded_tokenizer.model\"\n",
        "        st.sidebar.success(\"‚úÖ Model & Tokenizer loaded!\")\n",
        "\n",
        "\n",
        "# ------------------ Home ------------------\n",
        "def render_home():\n",
        "    st.title(\"üè† Welcome to TinyTransformer QA Playground\")\n",
        "    st.markdown(\"\"\"\n",
        "    A minimal transformer-based chatbot project to learn how GPT-like models work under the hood.\n",
        "\n",
        "    - Built from scratch using PyTorch\n",
        "    - Streamlit-powered dashboard\n",
        "    - Simple token-based Q&A transformer\n",
        "    \"\"\")\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"üß≠ Navigate using the sidebar to explore features\")\n",
        "\n",
        "# ------------------ Chat ------------------\n",
        "def render_qa_chat():\n",
        "    st.title(\"üí¨ Ask a Question\")\n",
        "    tab1, tab2 = st.tabs([\"üß† Ask\", \"üîç Explanation (coming soon)\"])\n",
        "\n",
        "    with tab1:\n",
        "        user_input = st.text_input(\"Type your question:\")\n",
        "        if st.button(\"üéØ Get Answer\") and user_input:\n",
        "            prompt = user_input\n",
        "            answer = answer_question(model, tokenizer,prompt).strip()\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            st.session_state.history.append({\n",
        "                \"question\": user_input,\n",
        "                \"answer\": answer,\n",
        "                \"timestamp\": timestamp\n",
        "            })\n",
        "            st.success(f\"ü§ñ {answer}\")\n",
        "\n",
        "    with tab2:\n",
        "        st.info(\"Model explanation and token-wise breakdown will go here in a future update!\")\n",
        "\n",
        "\n",
        "# ------------------ History ------------------\n",
        "def render_history():\n",
        "    st.title(\"üìú Conversation History\")\n",
        "    if st.session_state.history:\n",
        "        for item in st.session_state.history[::-1]:\n",
        "            with st.expander(f\"üïí {item['timestamp']}\"):\n",
        "                st.markdown(f\"**Q:** {item['question']}\")\n",
        "                st.markdown(f\"**A:** {item['answer']}\")\n",
        "    else:\n",
        "        st.info(\"No conversation history yet.\")\n",
        "\n",
        "    if st.button(\"üíæ Save Chat\"):\n",
        "        with open(\"chat_history.json\", \"w\") as f:\n",
        "            json.dump(st.session_state.history, f)\n",
        "        st.success(\"Saved to chat_history.json\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"üìÇ Load Previous History\", type=\"json\")\n",
        "    if uploaded_file:\n",
        "        st.session_state.history.extend(json.load(uploaded_file))\n",
        "        st.success(\"History loaded.\")\n",
        "\n",
        "# ------------------ Dataset ------------------\n",
        "def render_dataset():\n",
        "    import pandas as pd\n",
        "\n",
        "    st.title(\"üìö Dataset Viewer\")\n",
        "    tab1, tab2 = st.tabs([\"üîç Preview\", \"üì• Upload\"])\n",
        "\n",
        "    with tab1:\n",
        "        st.markdown(\"### üìÑ Dataset Preview\")\n",
        "\n",
        "        if \"uploaded_df\" in st.session_state:\n",
        "            st.success(\"Showing uploaded dataset:\")\n",
        "            st.dataframe(st.session_state.uploaded_df)\n",
        "        else:\n",
        "            st.info(\"No dataset uploaded yet. Showing static preview instead.\")\n",
        "            st.table([\n",
        "                {\"Question\": \"What is AI?\", \"Answer\": \"Artificial Intelligence\"},\n",
        "                {\"Question\": \"Who is Elon Musk?\", \"Answer\": \"Entrepreneur\"},\n",
        "                {\"Question\": \"What is Python?\", \"Answer\": \"Programming\"},\n",
        "            ])\n",
        "\n",
        "    with tab2:\n",
        "        import chromadb\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "\n",
        "        st.header(\"üìÇ Upload & Embed Your Knowledge\")\n",
        "\n",
        "        if \"doc_chunks\" not in st.session_state:\n",
        "            st.session_state.doc_chunks = []\n",
        "\n",
        "        embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        chroma_client = chromadb.Client()\n",
        "        if \"rag_memory\" not in [c.name for c in chroma_client.list_collections()]:\n",
        "            collection = chroma_client.create_collection(\"rag_memory\")\n",
        "        else:\n",
        "            collection = chroma_client.get_collection(\"rag_memory\")\n",
        "\n",
        "        data_file = st.file_uploader(\"Upload a .txt file for knowledge base\", type=[\"txt\"])\n",
        "        if data_file:\n",
        "          try:\n",
        "\n",
        "              df = pd.read_csv(data_file, sep=\"\\t\")\n",
        "\n",
        "              # Basic validation\n",
        "              if \"Question\" in df.columns and \"Answer\" in df.columns:\n",
        "                  st.session_state.uploaded_df = df[[\"Question\", \"Answer\"]].dropna()\n",
        "                  st.success(f\"‚úÖ Uploaded dataset with {len(st.session_state.uploaded_df)} valid QA pairs.\")\n",
        "              else:\n",
        "                  st.error(\"‚ùå The uploaded TSV must contain 'Question' and 'Answer' columns.\")\n",
        "          except Exception as e:\n",
        "              st.error(f\"‚ùå Failed to read TSV file: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------ Training Monitor ------------------\n",
        "def render_training():\n",
        "    import pandas as pd\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    import pickle\n",
        "    from torch.utils.data import DataLoader, TensorDataset\n",
        "    from utils import SimpleTokenizer, prepare_data\n",
        "    from model import TransformerEncoder, TinyQAModel\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    st.title(\"üìä Train Model Inside Streamlit\")\n",
        "\n",
        "    if \"uploaded_df\" not in st.session_state:\n",
        "        st.warning(\"Please upload a dataset first in the üìö Dataset tab.\")\n",
        "        return\n",
        "\n",
        "    df = st.session_state.uploaded_df\n",
        "    df.columns = [col.lower() for col in df.columns]\n",
        "\n",
        "    # Detect columns\n",
        "    if \"question\" in df.columns and \"answer\" in df.columns:\n",
        "        q_col, a_col = \"question\", \"answer\"\n",
        "    elif \"text\" in df.columns and \"response\" in df.columns:\n",
        "        q_col, a_col = \"text\", \"response\"\n",
        "    else:\n",
        "        st.error(\"Dataset must contain either ['question', 'answer'] or ['text', 'response']\")\n",
        "        return\n",
        "\n",
        "    st.subheader(\"‚öôÔ∏è Training Configuration\")\n",
        "    epochs = st.slider(\"Epochs\", 1, 20, 5)\n",
        "    batch_size = st.selectbox(\"Batch Size\", [2, 4, 8, 16], index=1)\n",
        "    lr = st.number_input(\"Learning Rate\", value=1e-3, format=\"%.5f\")\n",
        "    max_len = st.slider(\"Max Token Length\", 10, 100, 50)\n",
        "    st.subheader(\"‚öôÔ∏è Transformer Hyperparameters\")\n",
        "\n",
        "# üß© These are OUTSIDE the if-condition\n",
        "    d_model = st.slider(\"Model Size (d_model)\", 16, 128, 32, step=16)\n",
        "    num_heads = st.slider(\"Number of Attention Heads\", 1, 8, 2)\n",
        "    num_layers = st.slider(\"Number of Encoder Layers\", 1, 6, 2)\n",
        "    d_ff = st.slider(\"Feedforward Hidden Dim (d_ff)\", 32, 256, 64, step=32)\n",
        "    max_len = st.slider(\"Max Sequence Length\", 10, 100, 50)\n",
        "\n",
        "    # Optional model summary live preview\n",
        "    st.code(f\"\"\"\n",
        "    TinyQAModel(\n",
        "      encoder = TransformerEncoder(\n",
        "        layers = {num_layers},\n",
        "        heads = {num_heads},\n",
        "        d_model = {d_model},\n",
        "        d_ff = {d_ff},\n",
        "        max_len = {max_len}\n",
        "      ),\n",
        "      decoder = nn.Linear({d_model} ‚Üí vocab)\n",
        "    )\n",
        "    \"\"\", language=\"python\")\n",
        "\n",
        "    if st.button(\"üöÄ Train Model\"):\n",
        "        df = df[df[q_col].notna() & df[a_col].notna()]\n",
        "        df = df.astype({q_col: str, a_col: str})\n",
        "        uploaded_data = list(zip(df[q_col], df[a_col]))\n",
        "\n",
        "        chat_data = [\n",
        "            (\"Hi\", \"Hello!\"),\n",
        "            (\"Hey there\", \"Hi!\"),\n",
        "            (\"How are you?\", \"I'm doing well, thanks!\"),\n",
        "            (\"What's your name?\", \"I'm your chatbot.\"),\n",
        "            (\"What can you do?\", \"I answer questions.\"),\n",
        "            (\"Who made you?\", \"A student learning AI.\"),\n",
        "            (\"Tell me a joke\", \"Why did the computer get cold? It left its Windows open!\"),\n",
        "            (\"Nice joke\", \"Glad you liked it!\"),\n",
        "            (\"Do you like pizza?\", \"I love data... but pizza sounds good too.\"),\n",
        "            (\"What is AI?\", \"Artificial Intelligence.\"),\n",
        "            (\"What is ML?\", \"Machine Learning.\"),\n",
        "            (\"Tell me something cool\", \"Transformers power modern AI like ChatGPT!\"),\n",
        "            (\"Can you help me?\", \"Of course. What do you need?\"),\n",
        "            (\"Are you alive?\", \"Not yet üòÑ\"),\n",
        "            (\"Bye\", \"Goodbye!\"),\n",
        "            (\"Thanks\", \"You're welcome!\"),\n",
        "            (\"What‚Äôs 2 + 2?\", \"It‚Äôs 4.\"),\n",
        "            (\"What's the capital of India?\", \"New Delhi.\"),\n",
        "            (\"Can you sing?\", \"I can rhyme in binary.\"),\n",
        "            (\"What is Python?\", \"A programming language.\")\n",
        "        ]\n",
        "\n",
        "\n",
        "        # Combine base and uploaded dataset\n",
        "        combined_data = chat_data + uploaded_data\n",
        "\n",
        "        # ‚õî Clean\n",
        "        combined_data = [(q, a) for q, a in combined_data if isinstance(q, str) and isinstance(a, str)]\n",
        "\n",
        "        # ‚úÖ Split\n",
        "        train_data, val_data = train_test_split(combined_data, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Fit tokenizer on **all** data\n",
        "        tokenizer = BPETokenizer(\"chatbot_bpe.model\")\n",
        "        # Prepare tensors\n",
        "        X_train, y_train = prepare_data(train_data, tokenizer, max_len)\n",
        "        X_val, y_val = prepare_data(val_data, tokenizer, max_len)\n",
        "\n",
        "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
        "\n",
        "        vocab_size = tokenizer.sp.get_piece_size()\n",
        "\n",
        "        encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len=max_len)\n",
        "        model = TinyQAModel(encoder, d_model, vocab_size)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        loss_history = []\n",
        "\n",
        "        progress_bar = st.progress(0)\n",
        "        status_text = st.empty()\n",
        "        train_loss_history = []\n",
        "        val_loss_history = []\n",
        "        val_acc_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "          model.train()\n",
        "          total_loss = 0\n",
        "          for batch_x, batch_y in train_loader:\n",
        "              logits = model(batch_x)\n",
        "              logits = logits.view(-1, vocab_size)\n",
        "              targets = batch_y.view(-1)\n",
        "\n",
        "              loss = criterion(logits, targets)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              optimizer.zero_grad()\n",
        "              total_loss += loss.item()\n",
        "\n",
        "          avg_train_loss = total_loss / len(train_loader)\n",
        "          train_loss_history.append(avg_train_loss)\n",
        "\n",
        "          # üîç VALIDATION\n",
        "          model.eval()\n",
        "          val_loss = 0\n",
        "          correct = 0\n",
        "          total = 0\n",
        "          with torch.no_grad():\n",
        "              for val_x, val_y in val_loader:\n",
        "                  logits = model(val_x)\n",
        "                  logits = logits.view(-1, vocab_size)\n",
        "                  targets = val_y.view(-1)\n",
        "                  loss = criterion(logits, targets)\n",
        "                  val_loss += loss.item()\n",
        "\n",
        "                  preds = torch.argmax(logits, dim=1)\n",
        "                  correct += (preds == targets).sum().item()\n",
        "                  total += targets.size(0)\n",
        "\n",
        "          avg_val_loss = val_loss / len(val_loader)\n",
        "          val_acc = correct / total\n",
        "\n",
        "          val_loss_history.append(avg_val_loss)\n",
        "          val_acc_history.append(val_acc)\n",
        "\n",
        "          progress_bar.progress((epoch + 1) / epochs)\n",
        "          status_text.text(\n",
        "              f\"Epoch {epoch + 1}/{epochs} | \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f} | \"\n",
        "              f\"Val Acc: {val_acc:.2%}\"\n",
        "          )\n",
        "        torch.save(model.state_dict(), \"tinyqa_model.pth\")\n",
        "        with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "            pickle.dump(tokenizer, f)\n",
        "        misclassified = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for val_x, val_y in val_loader:\n",
        "                logits = model(val_x)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                for x, y_true, y_pred in zip(val_x, val_y, preds):\n",
        "                    if y_true != y_pred:\n",
        "                        question = tokenizer.decode([t for t in x.tolist() if t != 0])\n",
        "                        answer = tokenizer.decode([y_true.item()])\n",
        "                        predicted = tokenizer.decode([y_pred.item()])\n",
        "                        misclassified.append((question, answer, predicted))\n",
        "\n",
        "        if misclassified:\n",
        "            st.subheader(\"‚ùå Misclassified Examples\")\n",
        "            for q, true_a, pred_a in misclassified[:5]:  # limit to 5 for speed\n",
        "                st.markdown(f\"**Q:** {q}\")\n",
        "                st.markdown(f\"**True A:** {true_a} | **Pred A:** {pred_a}\")\n",
        "                st.markdown(\"---\")\n",
        "\n",
        "        st.success(\"‚úÖ Model trained and saved!\")\n",
        "        st.subheader(\"üìâ Training Progress\")\n",
        "\n",
        "        chart_data = {\n",
        "            \"Train Loss\": train_loss_history,\n",
        "            \"Val Loss\": val_loss_history,\n",
        "            \"Val Accuracy\": val_acc_history\n",
        "        }\n",
        "        st.line_chart(chart_data)\n",
        "        model_name = st.text_input(\"Model name to save (no extension)\", \"tinyqa_bpe\")\n",
        "        if st.button(\"üíæ Save Model\"):\n",
        "            torch.save(model.state_dict(), f\"{model_name}.pth\")\n",
        "            with open(f\"{model_name}_tokenizer.model\", \"wb\") as f:\n",
        "                f.write(open(\"chatbot_bpe.model\", \"rb\").read())\n",
        "            st.success(f\"Model and tokenizer saved as {model_name}.pth and {model_name}_tokenizer.model\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------ Settings ------------------\n",
        "def render_settings():\n",
        "    st.title(\"‚öôÔ∏è Model Settings\")\n",
        "    model_path = st.session_state.get(\"custom_model\", \"tinyqa_bpe_model.pth\")\n",
        "    tokenizer_path = st.session_state.get(\"custom_tokenizer\", \"chatbot_bpe.model\")\n",
        "\n",
        "\n",
        "    st.info(f\"\"\"\n",
        "    **Current Model**: `{model_path}`\n",
        "    **Tokenizer**: `{tokenizer_path}`\n",
        "    \"\"\")\n",
        "\n",
        "    if st.button(\"üîÅ Reset to Default\"):\n",
        "        st.session_state.custom_model = \"tinyqa_bpe_model.pth\"\n",
        "        st.session_state.custom_tokenizer = \"chatbot_bpe.model\"\n",
        "        st.success(\"Reset to default model.\")\n",
        "# ------------------ Architecture ------------------\n",
        "def render_architecture():\n",
        "    st.title(\"üß† Model Architecture\")\n",
        "    st.code(\"\"\"\n",
        "TinyQAModel(\n",
        "  encoder = TransformerEncoder(...),\n",
        "  decoder = nn.Linear(d_model ‚Üí vocab)\n",
        ")\n",
        "    \"\"\", language=\"python\")\n",
        "    st.markdown(\"Heads: 2 | Layers: 2 | d_model: 32 | max_len: 5\")\n",
        "\n",
        "# ------------------ About ------------------\n",
        "def render_about():\n",
        "    st.title(\"üìò About This App\")\n",
        "    st.markdown(\"\"\"\n",
        "This app is a **mini Transformer QA bot** built from scratch using PyTorch.\n",
        "\n",
        "**Built by:** Sumanth\n",
        "**Degree:** B.Tech CSE (AIML), Tier-3\n",
        "**Current Focus:** ML Engineering | Strategic AI Roles\n",
        "    \"\"\")\n",
        "    st.info(\"Built for learning and showcasing LLM mechanics. Updates coming soon!\")\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"üí° [GitHub Repo](#) | üß† Powered by PyTorch + Streamlit + SentencePiece\")\n",
        "\n",
        "# ------------------ Main ------------------\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"TinyQA Bot\", layout=\"wide\", initial_sidebar_state=\"collapsed\")\n",
        "    init_session()\n",
        "    section = render_sidebar()\n",
        "\n",
        "    if section == \"üè† Home\":\n",
        "        render_home()\n",
        "    elif section == \"üí¨ QA Chat\":\n",
        "        render_qa_chat()\n",
        "    elif section == \"üìú History\":\n",
        "        render_history()\n",
        "    elif section == \"üìö Dataset\":\n",
        "        render_dataset()\n",
        "    elif section == \"üìä Training Monitor\":\n",
        "        render_training()\n",
        "    elif section == \"‚öôÔ∏è Model Settings\":\n",
        "        render_settings()\n",
        "    elif section == \"üß† Architecture\":\n",
        "        render_architecture()\n",
        "    elif section == \"üìò About\":\n",
        "        render_about()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "illz1OQT95CI",
        "outputId": "ff3001ae-5062-4829-a467-365a3022e81d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "OKSOwMQL944u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03bd213f-731d-49a2-95fe-b438ea2c5764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting diagnostic_script.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile diagnostic_script.py\n",
        "import torch\n",
        "import pickle\n",
        "from model import TransformerEncoder, TinyQAModel\n",
        "from utils import BPETokenizer\n",
        "\n",
        "print(\"üîç Running diagnostics...\")\n",
        "\n",
        "# Load tokenizer\n",
        "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "print(f\"‚úì Tokenizer loaded\")\n",
        "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"  EOS token ID: {tokenizer.sp.eos_id()}\")\n",
        "print(f\"  PAD token ID: {tokenizer.vocab_size}\")\n",
        "\n",
        "# Test tokenizer\n",
        "test_text = \"Hello world\"\n",
        "encoded = tokenizer.encode(test_text)\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(f\"\\n‚úì Tokenizer test:\")\n",
        "print(f\"  Original: '{test_text}'\")\n",
        "print(f\"  Encoded: {encoded}\")\n",
        "print(f\"  Decoded: '{decoded}'\")\n",
        "\n",
        "# Load model\n",
        "max_len = 128\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "d_ff = 128\n",
        "pad_id=800\n",
        "num_layers = 2\n",
        "vocab_size = tokenizer.sp.get_piece_size()\n",
        "\n",
        "encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len=max_len)\n",
        "model = TinyQAModel(encoder, d_model, vocab_size)\n",
        "\n",
        "print(f\"\\n‚úì Model created\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Load weights\n",
        "try:\n",
        "    state_dict = torch.load(\"tinyqa_model.pth\", map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(state_dict,strict=False)\n",
        "    model.eval()\n",
        "    print(f\"‚úì Model weights loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Error loading model weights: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Test forward pass\n",
        "print(f\"\\nüß™ Testing model forward pass...\")\n",
        "test_input = \"Hi\"\n",
        "pad_id = tokenizer.sp.get_piece_size() - 1\n",
        "input_ids = tokenizer.encode(test_input)[:25] + [tokenizer.sp.eos_id()]\n",
        "padded = input_ids + [pad_id] * (max_len - len(input_ids))\n",
        "input_tensor = torch.tensor([padded], dtype=torch.long)\n",
        "\n",
        "print(f\"  Input text: '{test_input}'\")\n",
        "print(f\"  Input IDs: {input_ids}\")\n",
        "print(f\"  Input shape: {input_tensor.shape}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    logits = output\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    decoded_input = predicted_ids[0].tolist() if predicted_ids.ndim == 2 else predicted_ids.tolist()\n",
        "    response = tokenizer.decode(decoded_input)\n",
        "\n",
        "    print(f\"  Output shape: {output.shape}\")\n",
        "\n",
        "    # Check predictions at each position\n",
        "    print(f\"\\n  Token predictions:\")\n",
        "    for i in range(len(input_ids)):\n",
        "        token_logits = output[0, i, :]\n",
        "        top_tokens = torch.topk(token_logits, 5)\n",
        "        top_ids = top_tokens.indices.tolist()\n",
        "        top_probs = torch.softmax(top_tokens.values, dim=-1).tolist()\n",
        "\n",
        "        print(f\"    Position {i}: {tokenizer.decode([padded[i]])} ->\")\n",
        "        for tid, prob in zip(top_ids, top_probs):\n",
        "            decoded = tokenizer.decode([tid])\n",
        "            print(f\"      {decoded}: {prob:.3f}\")\n",
        "\n",
        "# Test generation\n",
        "print(f\"\\nüß™ Testing generation...\")\n",
        "\n",
        "def debug_generate(question):\n",
        "    q_ids = tokenizer.encode(question)[:25]\n",
        "    input_ids = q_ids + [tokenizer.sp.eos_id()]\n",
        "\n",
        "    print(f\"  Question: '{question}'\")\n",
        "    print(f\"  Question IDs: {q_ids}\")\n",
        "\n",
        "    generated = []\n",
        "    with torch.no_grad():\n",
        "        for step in range(10):  # Generate up to 10 tokens\n",
        "            current_seq = input_ids + generated\n",
        "            padded = current_seq + [tokenizer.sp.pad_id()] * (max_len - len(current_seq))\n",
        "            input_tensor = torch.tensor([padded], dtype=torch.long)\n",
        "\n",
        "            output = model(input_tensor)\n",
        "            next_pos = len(current_seq) - 1\n",
        "\n",
        "            if next_pos >= max_len - 1:\n",
        "                break\n",
        "\n",
        "            logits = output[0, next_pos, :]\n",
        "            next_token = torch.argmax(logits).item()\n",
        "\n",
        "            print(f\"    Step {step}: pos={next_pos}, next_token={next_token} ('{tokenizer.decode([next_token])}')\")\n",
        "\n",
        "            if next_token == tokenizer.sp.eos_id() or next_token == tokenizer.sp.pad_id():\n",
        "                print(f\"    Stopping: hit EOS/PAD\")\n",
        "                break\n",
        "\n",
        "            generated.append(next_token)\n",
        "\n",
        "    if generated:\n",
        "        answer = tokenizer.decode(generated)\n",
        "        print(f\"  Generated tokens: {generated}\")\n",
        "        print(f\"  Answer: '{answer}'\")\n",
        "    else:\n",
        "        print(f\"  No tokens generated!\")\n",
        "\n",
        "# Test with different questions\n",
        "for q in [\"Hi\", \"Hello\", \"What's your name?\"]:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    debug_generate(q)\n",
        "from inference import answer_question\n",
        "\n",
        "print(\"\\nüö® Testing live inference:\")\n",
        "qs = [\"Hi\", \"Hello\", \"What's your name?\"]\n",
        "for q in qs:\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {answer_question(model,tokenizer,q)}\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Diagnostics complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "tuvEMa9u942Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e95bdd7-eb05-4494-8b9a-0843414f6c77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-02 14:19:18.565658: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754144358.590390   86646 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754144358.597695   86646 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "üîç Running diagnostics...\n",
            "‚úì Tokenizer loaded\n",
            "  Vocab size: 800\n",
            "  EOS token ID: 2\n",
            "  PAD token ID: 800\n",
            "\n",
            "‚úì Tokenizer test:\n",
            "  Original: 'Hello world'\n",
            "  Encoded: [335, 30, 283]\n",
            "  Decoded: 'Hello world'\n",
            "\n",
            "‚úì Model created\n",
            "  Total parameters: 170,337\n",
            "‚úì Model weights loaded successfully\n",
            "\n",
            "üß™ Testing model forward pass...\n",
            "  Input text: 'Hi'\n",
            "  Input IDs: [251, 2]\n",
            "  Input shape: torch.Size([1, 128])\n",
            "  Output shape: torch.Size([1, 128, 801])\n",
            "\n",
            "  Token predictions:\n",
            "    Position 0: Hi ->\n",
            "      Hello: 0.744\n",
            "      H: 0.070\n",
            "      Qu: 0.067\n",
            "      T: 0.063\n",
            "      As: 0.056\n",
            "    Position 1:  ->\n",
            "      there: 0.254\n",
            "      ning: 0.215\n",
            "      !: 0.199\n",
            "      atile: 0.176\n",
            "      that: 0.155\n",
            "\n",
            "üß™ Testing generation...\n",
            "\n",
            "==================================================\n",
            "  Question: 'Hi'\n",
            "  Question IDs: [251]\n",
            "    Step 0: pos=1, next_token=769 ('!')\n",
            "    Step 1: pos=2, next_token=765 (''')\n",
            "    Step 2: pos=3, next_token=765 (''')\n",
            "    Step 3: pos=4, next_token=765 (''')\n",
            "    Step 4: pos=5, next_token=765 (''')\n",
            "    Step 5: pos=6, next_token=765 (''')\n",
            "    Step 6: pos=7, next_token=765 (''')\n",
            "    Step 7: pos=8, next_token=765 (''')\n",
            "    Step 8: pos=9, next_token=765 (''')\n",
            "    Step 9: pos=10, next_token=765 (''')\n",
            "  Generated tokens: [769, 765, 765, 765, 765, 765, 765, 765, 765, 765]\n",
            "  Answer: '!''''''''''\n",
            "\n",
            "==================================================\n",
            "  Question: 'Hello'\n",
            "  Question IDs: [335]\n",
            "    Step 0: pos=1, next_token=344 ('there')\n",
            "    Step 1: pos=2, next_token=12 ('is')\n",
            "    Step 2: pos=3, next_token=12 ('is')\n",
            "    Step 3: pos=4, next_token=12 ('is')\n",
            "    Step 4: pos=5, next_token=500 ('ause')\n",
            "    Step 5: pos=6, next_token=765 (''')\n",
            "    Step 6: pos=7, next_token=765 (''')\n",
            "    Step 7: pos=8, next_token=765 (''')\n",
            "    Step 8: pos=9, next_token=765 (''')\n",
            "    Step 9: pos=10, next_token=765 (''')\n",
            "  Generated tokens: [344, 12, 12, 12, 500, 765, 765, 765, 765, 765]\n",
            "  Answer: 'there is is isause''''''\n",
            "\n",
            "==================================================\n",
            "  Question: 'What's your name?'\n",
            "  Question IDs: [21, 765, 740, 332, 624, 756]\n",
            "    Step 0: pos=6, next_token=332 ('your')\n",
            "    Step 1: pos=7, next_token=45 ('T')\n",
            "    Step 2: pos=8, next_token=543 ('You')\n",
            "    Step 3: pos=9, next_token=647 ('istant')\n",
            "    Step 4: pos=10, next_token=754 ('.')\n",
            "    Step 5: pos=11, next_token=765 (''')\n",
            "    Step 6: pos=12, next_token=765 (''')\n",
            "    Step 7: pos=13, next_token=765 (''')\n",
            "    Step 8: pos=14, next_token=765 (''')\n",
            "    Step 9: pos=15, next_token=765 (''')\n",
            "  Generated tokens: [332, 45, 543, 647, 754, 765, 765, 765, 765, 765]\n",
            "  Answer: 'your T Youistant.''''''\n",
            "\n",
            "üö® Testing live inference:\n",
            "Q: Hi\n",
            "üîé Step 0: token 152 ‚Üí 'and'\n",
            "üîé Step 1: token 151 ‚Üí 'The'\n",
            "üîé Step 2: token 192 ‚Üí 'Pyt'\n",
            "üîé Step 3: token 627 ‚Üí 'reus'\n",
            "üîé Step 4: token 599 ‚Üí 'Good'\n",
            "üîé Step 5: token 715 ‚Üí 'humans'\n",
            "üîé Step 6: token 37 ‚Üí 'A'\n",
            "üîé Step 7: token 227 ‚Üí 'ans'\n",
            "üîé Step 8: token 749 ‚Üí 'm'\n",
            "üîé Step 9: token 661 ‚Üí 'build'\n",
            "üîé Step 10: token 84 ‚Üí 'g'\n",
            "üîé Step 11: token 769 ‚Üí '!'\n",
            "üîé Step 12: token 724 ‚Üí 'nerves'\n",
            "üîé Step 13: token 603 ‚Üí 'Take'\n",
            "üîé Step 14: token 12 ‚Üí 'is'\n",
            "üîé Step 15: token 373 ‚Üí 'smartphone'\n",
            "üîé Step 16: token 622 ‚Üí 'mole'\n",
            "üîé Step 17: token 706 ‚Üí 'during'\n",
            "üîé Step 18: token 297 ‚Üí 'con'\n",
            "üîé Step 19: token 96 ‚Üí 'ct'\n",
            "üîé Step 20: token 473 ‚Üí 'As'\n",
            "üîé Step 21: token 359 ‚Üí 'process'\n",
            "üîé Step 22: token 117 ‚Üí 'was'\n",
            "üîé Step 23: token 23 ‚Üí 'le'\n",
            "üîé Step 24: token 668 ‚Üí 'featu'\n",
            "üîé Step 25: token 281 ‚Üí 'loud'\n",
            "üîé Step 26: token 648 ‚Üí 'vanced'\n",
            "üîé Step 27: token 556 ‚Üí 'has'\n",
            "üîé Step 28: token 356 ‚Üí 'capital'\n",
            "üîé Step 29: token 149 ‚Üí 'ence'\n",
            "üîé Step 30: token 515 ‚Üí 'iden'\n",
            "üîé Step 31: token 784 ‚Üí 'U'\n",
            "üîé Step 32: token 61 ‚Üí 'ut'\n",
            "üîé Step 33: token 232 ‚Üí 'eed'\n",
            "üîé Step 34: token 578 ‚Üí 'erson'\n",
            "üîé Step 35: token 280 ‚Üí 'king'\n",
            "üîé Step 36: token 47 ‚Üí 'st'\n",
            "üîé Step 37: token 515 ‚Üí 'iden'\n",
            "üîé Step 38: token 349 ‚Üí 'Python'\n",
            "üîé Step 39: token 673 ‚Üí 'learn'\n",
            "üîé Step 40: token 325 ‚Üí 'make'\n",
            "üîé Step 41: token 329 ‚Üí 'used'\n",
            "üîé Step 42: token 494 ‚Üí 'ants'\n",
            "üîé Step 43: token 602 ‚Üí 'Sure'\n",
            "üîé Step 44: token 84 ‚Üí 'g'\n",
            "üîé Step 45: token 275 ‚Üí 'ific'\n",
            "üîé Step 46: token 227 ‚Üí 'ans'\n",
            "üîé Step 47: token 603 ‚Üí 'Take'\n",
            "üîé Step 48: token 19 ‚Üí 'ing'\n",
            "üîé Step 49: token 204 ‚Üí 'smart'\n",
            "üß™ Raw decoded: 'and The Pyt reus Good humans Aansm build g! nerves Take is smartphone mole during conct As process wasle featuloudvanced has capitalenceidenUuteedersonking stiden Python learn make usedants Sure gificans Takeing smart'\n",
            "üßπ Cleaned answer: 'and The Pyt reus Good humans Aansm build g! nerves Take is smartphone mole during conct As process wasle featuloudvanced has capitalenceidenUuteedersonking stiden Python learn make usedants Sure gificans Takeing smart'\n",
            "A: and The Pyt reus Good humans Aansm build g! nerves Take is smartphone mole during conct As process wasle featuloudvanced has capitalenceidenUuteedersonking stiden Python learn make usedants Sure gificans Takeing smart\n",
            "\n",
            "Q: Hello\n",
            "üîé Step 0: token 251 ‚Üí 'Hi'\n",
            "üîé Step 1: token 335 ‚Üí 'Hello'\n",
            "üîé Step 2: token 75 ‚Üí 'ning'\n",
            "üîé Step 3: token 595 ‚Üí '1939'\n",
            "üîé Step 4: token 207 ‚Üí 'state'\n",
            "üîé Step 5: token 54 ‚Üí 'to'\n",
            "üîé Step 6: token 489 ‚Üí 'we'\n",
            "üîé Step 7: token 522 ‚Üí 'leep'\n",
            "üîé Step 8: token 219 ‚Üí 'ue'\n",
            "üîé Step 9: token 251 ‚Üí 'Hi'\n",
            "üîé Step 10: token 725 ‚Üí 'person'\n",
            "üîé Step 11: token 280 ‚Üí 'king'\n",
            "üîé Step 12: token 771 ‚Üí 'P'\n",
            "üîé Step 13: token 630 ‚Üí 'spec'\n",
            "üîé Step 14: token 455 ‚Üí 'ory'\n",
            "üîé Step 15: token 660 ‚Üí 'block'\n",
            "üîé Step 16: token 371 ‚Üí 'ransformers'\n",
            "üîé Step 17: token 124 ‚Üí 'ec'\n",
            "üîé Step 18: token 217 ‚Üí 'lp'\n",
            "üîé Step 19: token 326 ‚Üí 'math'\n",
            "üîé Step 20: token 352 ‚Üí 'living'\n",
            "üîé Step 21: token 290 ‚Üí 'Ein'\n",
            "üîé Step 22: token 333 ‚Üí 'apital'\n",
            "üîé Step 23: token 0 ‚Üí ''\n",
            "üõë Decoding stopped (EOS or PAD)\n",
            "üß™ Raw decoded: 'Hi Helloning 1939 state to weleepue Hi personkingP specory blockransformerseclp math living Einapital'\n",
            "üßπ Cleaned answer: 'Hi Helloning 1939 state to weleepue Hi personkingP specory blockransformerseclp math living Einapital'\n",
            "A: Hi Helloning 1939 state to weleepue Hi personkingP specory blockransformerseclp math living Einapital\n",
            "\n",
            "Q: What's your name?\n",
            "üîé Step 0: token 364 ‚Üí 'networks'\n",
            "üîé Step 1: token 620 ‚Üí 'mass'\n",
            "üîé Step 2: token 713 ‚Üí 'global'\n",
            "üîé Step 3: token 351 ‚Üí 'access'\n",
            "üîé Step 4: token 734 ‚Üí 'strong'\n",
            "üîé Step 5: token 377 ‚Üí '2?'\n",
            "üîé Step 6: token 344 ‚Üí 'there'\n",
            "üîé Step 7: token 12 ‚Üí 'is'\n",
            "üîé Step 8: token 373 ‚Üí 'smartphone'\n",
            "üîé Step 9: token 765 ‚Üí '''\n",
            "üîé Step 10: token 765 ‚Üí '''\n",
            "üîé Step 11: token 703 ‚Üí 'branch'\n",
            "üîé Step 12: token 641 ‚Üí 'atters'\n",
            "üîé Step 13: token 740 ‚Üí 's'\n",
            "üîé Step 14: token 555 ‚Üí 'han'\n",
            "üîé Step 15: token 114 ‚Üí 'for'\n",
            "üîé Step 16: token 673 ‚Üí 'learn'\n",
            "üîé Step 17: token 652 ‚Üí 'Cells'\n",
            "üîé Step 18: token 356 ‚Üí 'capital'\n",
            "üîé Step 19: token 24 ‚Üí 'of'\n",
            "üîé Step 20: token 776 ‚Üí 'C'\n",
            "üîé Step 21: token 302 ‚Üí 'not'\n",
            "üîé Step 22: token 600 ‚Üí 'Mach'\n",
            "üîé Step 23: token 122 ‚Üí 'am'\n",
            "üîé Step 24: token 672 ‚Üí 'langu'\n",
            "üîé Step 25: token 477 ‚Üí 'Qu'\n",
            "üîé Step 26: token 244 ‚Üí 'lou'\n",
            "üîé Step 27: token 361 ‚Üí 'quantum'\n",
            "üîé Step 28: token 39 ‚Üí 'st'\n",
            "üîé Step 29: token 692 ‚Üí 'cluding'\n",
            "üîé Step 30: token 790 ‚Üí '‚Äô'\n",
            "üîé Step 31: token 201 ‚Üí 'than'\n",
            "üîé Step 32: token 639 ‚Üí 'angled'\n",
            "üîé Step 33: token 370 ‚Üí 'statement'\n",
            "üîé Step 34: token 477 ‚Üí 'Qu'\n",
            "üîé Step 35: token 641 ‚Üí 'atters'\n",
            "üîé Step 36: token 33 ‚Üí 'd'\n",
            "üîé Step 37: token 723 ‚Üí 'mobile'\n",
            "üîé Step 38: token 53 ‚Üí 'ion'\n",
            "üîé Step 39: token 7 ‚Üí 'a'\n",
            "üîé Step 40: token 401 ‚Üí 'rt'\n",
            "üîé Step 41: token 37 ‚Üí 'A'\n",
            "üîé Step 42: token 758 ‚Üí 'b'\n",
            "üîé Step 43: token 293 ‚Üí 'Why'\n",
            "üîé Step 44: token 247 ‚Üí 'ors'\n",
            "üîé Step 45: token 547 ‚Üí 'bur'\n",
            "üîé Step 46: token 195 ‚Üí 'antum'\n",
            "üîé Step 47: token 39 ‚Üí 'st'\n",
            "üîé Step 48: token 646 ‚Üí 'inyBot'\n",
            "üîé Step 49: token 37 ‚Üí 'A'\n",
            "üß™ Raw decoded: 'networks mass global access strong2? there is smartphone'' branchatterss han for learn Cells capital ofC not Macham langu Qulou quantumstcluding‚Äô thanangled statement Quatters d mobileion art Ab Whyors burantumstinyBot A'\n",
            "üßπ Cleaned answer: 'networks mass global access strong2? there is smartphone'' branchatterss han for learn Cells capital ofC not Macham langu Qulou quantumstcluding‚Äô thanangled statement Quatters d mobileion art Ab Whyors burantumstinyBot A'\n",
            "A: networks mass global access strong2? there is smartphone'' branchatterss han for learn Cells capital ofC not Macham langu Qulou quantumstcluding‚Äô thanangled statement Quatters d mobileion art Ab Whyors burantumstinyBot A\n",
            "\n",
            "\n",
            "‚úÖ Diagnostics complete!\n"
          ]
        }
      ],
      "source": [
        "!python diagnostic_script.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "VNQ8cEbB94xP"
      },
      "outputs": [],
      "source": [
        "!rm chatbot_bpe.*  # Delete old model if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5NgUVyT94mB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGlVjo23L-dB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcaFEhsbSx2vlhpMpdgOg0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}