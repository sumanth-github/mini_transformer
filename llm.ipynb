{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumanth-github/mini_transformer/blob/main/llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o7vLo55YUGI-"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb\n",
        "!pip install tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ahwOq6B3bct4"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TQWa9lKLdfQ",
        "outputId": "cd5bed32-e692-4c85-b793-05e0235f468c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2x2czTq2vshQKZ0WK5yTPHJM7Ml_2KSCazfnhEDPtWrnuKHom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "06NA91r6bf4I",
        "outputId": "df80cde5-61cc-4cd5-f676-378363b0f977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.47.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.47.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.47.1 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jhR7n9W95Tj",
        "outputId": "ca40af52-5abc-446d-855a-e9b668dec895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your app is live at: NgrokTunnel: \"https://8531403103f4.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import streamlit as st\n",
        "\n",
        "# Write your app.py content\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write('''\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"Tiny LLM Chatbot\")\n",
        "user_input = st.text_input(\"You: \")\n",
        "\n",
        "if user_input:\n",
        "    # Dummy response logic, replace with real model\n",
        "    st.write(\"Bot:\", \"This is a placeholder response.\")\n",
        "''')\n",
        "\n",
        "# Start the Streamlit app\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "\n",
        "# Expose it with ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Your app is live at:\", public_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "# config.py\n",
        "MAX_LEN    = 128\n",
        "D_MODEL    = 64\n",
        "NUM_HEADS  = 4\n",
        "D_FF       = 128\n",
        "NUM_LAYERS = 3\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 100\n",
        "LR         = 5e-4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj2LZqy6QwG5",
        "outputId": "caa1a8e7-209c-43de-dd11-aa15a71b3961"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpx22dXU95N7",
        "outputId": "0f782085-2282-47b4-9bbe-3eac4fc1c9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Positional Encoding Layer\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1), :].to(x.device)\n",
        "\n",
        "\n",
        "# One Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        ff_out = self.ff(x)\n",
        "        return self.norm2(x + self.dropout(ff_out))\n",
        "\n",
        "# Full Transformer Encoder (stack of layers)\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size + 1, d_model, padding_idx=0)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x + self.pos_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# TinyQA Model: Encoder + Decoder (CLS-style output)\n",
        "class TinyQAModel(nn.Module):\n",
        "    def __init__(self, encoder, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = nn.Linear(d_model, vocab_size)  # Project [CLS] token output to vocab\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)           # shape: (batch, seq_len, d_model)\n",
        "        return self.decoder(encoded)      # output: (batch, vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2ZHT_bB95Lh",
        "outputId": "81be53a2-460d-4efe-a974-4656d0aafa40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils.py\n",
        "import torch\n",
        "from chromadb import Client\n",
        "import chromadb.utils.embedding_functions as embedding_functions\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Initialize embedder\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "chroma_client = Client()\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"rag_memory\")\n",
        "\n",
        "def embed_chunks(chunks):\n",
        "    embeddings = embedder.encode(chunks, show_progress_bar=True)\n",
        "    ids = [f\"id_{i}\" for i in range(len(chunks))]\n",
        "    chroma_collection.add(documents=chunks, embeddings=embeddings, ids=ids)\n",
        "    return len(chunks)\n",
        "\n",
        "def build_rag_prompt(user_query, top_k=3):\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import chromadb\n",
        "    chroma_client = chromadb.Client()\n",
        "    collection = chroma_client.get_or_create_collection(\"rag_memory\")\n",
        "\n",
        "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    query_embedding = embedder.encode([user_query])[0]\n",
        "\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    context_chunks = results[\"documents\"][0]\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "\n",
        "    return f\"Context:\\n{context}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
        "\n",
        "\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1, \"<EOS>\": 2}\n",
        "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\", 2: \"<EOS>\"}\n",
        "        self.vocab_size = 3\n",
        "\n",
        "    def fit(self, texts):\n",
        "        for text in texts:\n",
        "            for word in text.strip().split():\n",
        "                if word not in self.word2idx:\n",
        "                    idx = len(self.word2idx)\n",
        "                    self.word2idx[word] = idx\n",
        "                    self.idx2word[idx] = word\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "\n",
        "    def encode(self, text, max_len=None, eos=True):\n",
        "        tokens = [self.word2idx.get(word, 1) for word in text.strip().split()]\n",
        "        if eos:\n",
        "            tokens.append(2)\n",
        "        if max_len:\n",
        "            tokens = tokens[:max_len] + [0] * max(0, max_len - len(tokens))\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return \" \".join([self.idx2word.get(token, \"<UNK>\") for token in tokens if token != 0])\n",
        "\n",
        "    def get_pad_id(self):\n",
        "        return 0\n",
        "\n",
        "    def get_eos_id(self):\n",
        "        return 2\n",
        "\n",
        "def prepare_data(pairs, tokenizer, max_len, pad_id):\n",
        "    inputs, targets = [], []\n",
        "    for q, a in pairs:\n",
        "        q_ids = tokenizer.encode(q)[:max_len//2]\n",
        "        a_ids = tokenizer.encode(a)[:max_len//2]\n",
        "        if hasattr(tokenizer, 'sp'):\n",
        "            eos_id = tokenizer.sp.eos_id()\n",
        "        else:\n",
        "            eos_id = 1  # fallback for SimpleTokenizer (or define a constant)\n",
        "\n",
        "        input_seq = q_ids + [eos_id]\n",
        "        target_seq = a_ids + [eos_id]\n",
        "\n",
        "        # Pad sequences\n",
        "        input_seq += [pad_id] * (max_len - len(input_seq))\n",
        "        target_seq += [pad_id] * (max_len - len(target_seq))\n",
        "\n",
        "        inputs.append(input_seq)\n",
        "        targets.append(target_seq)\n",
        "\n",
        "    X = torch.tensor(inputs, dtype=torch.long)\n",
        "    y = torch.tensor(targets, dtype=torch.long)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BPETokenizer:\n",
        "    def __init__(self, model_file=\"chatbot_bpe.model\"):\n",
        "        import sentencepiece as spm\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.load(model_file)\n",
        "\n",
        "    def encode(self, text, max_len=None):\n",
        "        tokens = self.sp.encode(text, out_type=int)\n",
        "        if max_len:\n",
        "            tokens = tokens[:max_len] + [0] * max(0, max_len - len(tokens))\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        return self.sp.decode(token_ids).replace(\"▁\", \" \").strip()\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return self.sp.get_piece_size()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "1zU4e1y3iRqL"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "PmwncOcQ94_b",
        "outputId": "51e92db6-fb7f-4d85-a8ef-a44b16bbf8a4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-da8d7048-c8ca-485d-8846-a1347ebe3f4a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-da8d7048-c8ca-485d-8846-a1347ebe3f4a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving augmented_qa_dataset.json to augmented_qa_dataset.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "ewft3hbk948-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"augmented_qa_dataset.json\", \"r\") as f:\n",
        "    qa_pairs = json.load(f)\n",
        "\n",
        "with open(\"bpe_train.txt\", \"w\") as f_out:\n",
        "    for q, a in qa_pairs:\n",
        "        f_out.write(q.strip() + \"\\n\")\n",
        "        f_out.write(a.strip() + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiHI0WeX946_",
        "outputId": "60f7d21a-9936-4166-b646-79bede311b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=\"bpe_train.txt\",\n",
        "    model_prefix=\"chatbot_bpe\",\n",
        "    vocab_size=800,\n",
        "    model_type=\"bpe\",\n",
        "    character_coverage=1.0,\n",
        "    pad_id=0,\n",
        "    unk_id=1,\n",
        "    bos_id=-1,  # disables BOS\n",
        "    eos_id=2,\n",
        "    hard_vocab_limit=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MKKXd6H95Jc",
        "outputId": "2f4345fe-4133-485e-f031-a3c22c47b1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train.py\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from model import TransformerEncoder, TinyQAModel\n",
        "from utils import BPETokenizer, prepare_data\n",
        "\n",
        "# ==== Load the full dataset ====\n",
        "import json\n",
        "with open(\"augmented_qa_dataset.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(data)} Q&A pairs\")\n",
        "\n",
        "# Add some basic conversational examples\n",
        "conversational_data = [\n",
        "    (\"Hi\", \"Hello!\"),\n",
        "    (\"Hello\", \"Hi there!\"),\n",
        "    (\"Hey\", \"Hey! How can I help you?\"),\n",
        "    (\"How are you?\", \"I'm doing well, thanks for asking!\"),\n",
        "    (\"What's your name?\", \"I'm TinyBot, your AI assistant.\"),\n",
        "    (\"Thank you\", \"You're welcome!\"),\n",
        "    (\"Thanks\", \"Happy to help!\"),\n",
        "    (\"Bye\", \"Goodbye!\"),\n",
        "    (\"Goodbye\", \"See you later!\"),\n",
        "]\n",
        "\n",
        "# Combine datasets\n",
        "data = conversational_data + data  # Use first 100 from dataset to keep training manageable\n",
        "\n",
        "# ==== Hyperparameters ====\n",
        "max_len = 128\n",
        "d_model = 64  # Increased for better capacity\n",
        "num_heads = 4\n",
        "d_ff = 128\n",
        "num_layers = 3  # More layers for better understanding\n",
        "batch_size = 8\n",
        "num_epochs = 100  # More epochs for better learning\n",
        "learning_rate = 5e-4\n",
        "\n",
        "# ==== Tokenizer & Data ====\n",
        "tokenizer = BPETokenizer(\"chatbot_bpe.model\")\n",
        "eos_id = tokenizer.sp.eos_id()\n",
        "pad_id = tokenizer.sp.pad_id()\n",
        "vocab_size=tokenizer.sp.get_piece_size()\n",
        "print(f\"🧠 Vocab size: {vocab_size}, PAD: {pad_id}, EOS: {eos_id}\")\n",
        "\n",
        "# Prepare data\n",
        "X, y = prepare_data(data, tokenizer, max_len, pad_id)\n",
        "dataset = TensorDataset(X, y)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# ==== Model ====\n",
        "encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len=max_len)\n",
        "model = TinyQAModel(encoder, d_model, vocab_size)\n",
        "\n",
        "# ==== Training ====\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)  # Ignore padding\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "loss_history = []\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_x, batch_y in loader:\n",
        "        logits = model(batch_x)  # shape: [batch, seq_len, vocab_size]\n",
        "        logits = logits.view(-1, vocab_size)  # [batch * seq_len, vocab_size]\n",
        "        targets = batch_y.view(-1)            # [batch * seq_len]\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(logits, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    loss_history.append(avg_loss)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {avg_loss:.4f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "# Save model and tokenizer\n",
        "torch.save(model.state_dict(), \"tinyqa_model.pth\")\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(\"✅ Training complete. Model saved.\")\n",
        "\n",
        "# Test the model\n",
        "print(\"\\n🧪 Testing the model:\")\n",
        "test_questions = [\"Hi\", \"What's your name?\", \"Tell me a joke\", \"Thanks\"]\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for q in test_questions:\n",
        "        input_ids = tokenizer.encode(q)[:max_len]\n",
        "        input_len=len(input_ids)\n",
        "        generated = input_ids + [pad_id] * (max_len - len(input_ids))\n",
        "\n",
        "        for step in range(20):\n",
        "            input_tensor = torch.tensor([generated[:max_len]], dtype=torch.long)\n",
        "            logits = model(input_tensor)\n",
        "            next_pos = input_len + step\n",
        "            if next_pos >= max_len:\n",
        "                break\n",
        "            token_logits = logits[0, next_pos - 1, :]\n",
        "            next_token_id = torch.argmax(token_logits).item()\n",
        "            if next_token_id in [pad_id, eos_id]:\n",
        "                break\n",
        "            generated[next_pos] = next_token_id\n",
        "\n",
        "        answer_ids = generated[input_len:next_pos]\n",
        "        answer = tokenizer.decode(answer_ids) if answer_ids else \"[No response]\"\n",
        "        print(f\"Q: {q} → A: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqVR-K1ojUV6",
        "outputId": "612269cb-22d4-4ae5-88be-5213d3e9b060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-02 15:17:16.234506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754147836.307637  100776 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754147836.330150  100776 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loaded 44 Q&A pairs\n",
            "🧠 Vocab size: 800, PAD: 0, EOS: 2\n",
            "Starting training...\n",
            "Epoch 10/100: Loss = 5.1903, LR = 0.000500\n",
            "Epoch 20/100: Loss = 4.4705, LR = 0.000250\n",
            "Epoch 30/100: Loss = 4.1436, LR = 0.000250\n",
            "Epoch 40/100: Loss = 3.8452, LR = 0.000125\n",
            "Epoch 50/100: Loss = 3.6576, LR = 0.000125\n",
            "Epoch 60/100: Loss = 3.4903, LR = 0.000063\n",
            "Epoch 70/100: Loss = 3.4326, LR = 0.000063\n",
            "Epoch 80/100: Loss = 3.3602, LR = 0.000031\n",
            "Epoch 90/100: Loss = 3.3015, LR = 0.000031\n",
            "Epoch 100/100: Loss = 3.2647, LR = 0.000016\n",
            "✅ Training complete. Model saved.\n",
            "\n",
            "🧪 Testing the model:\n",
            "Q: Hi → A: Hello Hi Hello Hi Hello Hi Hello Hi Hello Hi Hello Hi Hello Hi Hello Hi Hello Hi Hello\n",
            "Q: What's your name? → A: , a is of\n",
            "Q: Tell me a joke → A: t a is is is is of 42. is is a is a is of\n",
            "Q: Thanks → A: H H H H H H H H H H H H H H\n"
          ]
        }
      ],
      "source": [
        "!python train.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaE5gMrg95G3",
        "outputId": "fc419e8e-342b-4acc-b38b-59b9efe4efb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting inference.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile inference.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "from model import TransformerEncoder, TinyQAModel\n",
        "from utils import BPETokenizer\n",
        "\n",
        "# ==== Load tokenizer & hyperparams ====\n",
        "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "# Must match training parameters\n",
        "max_len = 128\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "d_ff = 128\n",
        "num_layers = 3\n",
        "\n",
        "# ==== Load model ====\n",
        "vocab_size = tokenizer.sp.get_piece_size()\n",
        "encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len=max_len)\n",
        "model = TinyQAModel(encoder, d_model, vocab_size)\n",
        "\n",
        "# Load state dict\n",
        "state_dict = torch.load(\"tinyqa_model.pth\", map_location=torch.device('cpu'))\n",
        "for k, v in state_dict.items():\n",
        "    if 'embedding.weight' in k:\n",
        "        print(f\"📏 From checkpoint: {k} = {v.shape}\")\n",
        "print(f\"📏 Your model: embedding.weight = {model.encoder.embedding.weight.shape}\")\n",
        "\n",
        "state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"encoder.pos_encoding.pe\")}\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "\n",
        "def clean_decode(tokens, tokenizer):\n",
        "    words = tokenizer.decode(tokens).split()\n",
        "    cleaned = []\n",
        "    for w in words:\n",
        "        if w not in (\"<PAD>\", \"<UNK>\", \"NULL\"):\n",
        "            if not cleaned or cleaned[-1] != w:\n",
        "                cleaned.append(w)\n",
        "    return \" \".join(cleaned).strip()\n",
        "\n",
        "def answer_question(model, tokenizer, prompt: str, max_gen_len=50, temperature=0.7, max_len=128):\n",
        "    persona_prompt = \"You are a witty and helpful assistant. Answer briefly.\\n\"\n",
        "    input_text = persona_prompt + prompt\n",
        "    input_ids = tokenizer.encode(input_text)[:max_len]\n",
        "    input_len = len([x for x in input_ids if x != 0])\n",
        "\n",
        "    generated = input_ids.copy()\n",
        "    eos_id = tokenizer.sp.eos_id()\n",
        "    pad_id = tokenizer.sp.pad_id()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step in range(max_gen_len):\n",
        "            input_tensor = torch.tensor([generated[:max_len]], dtype=torch.long)\n",
        "            logits = model(input_tensor)\n",
        "            pos = len(generated) - 1\n",
        "            if pos >= max_len - 1:\n",
        "                break\n",
        "\n",
        "            next_token_logits = logits[0, pos, :] / temperature\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_token_id = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            print(f\"🔎 Step {step}: token {next_token_id} → '{tokenizer.decode([next_token_id])}'\")\n",
        "\n",
        "            if next_token_id == eos_id or next_token_id == pad_id:\n",
        "                print(\"🚩 Decoding stopped (EOS or PAD)\")\n",
        "                break\n",
        "\n",
        "            generated.append(next_token_id)\n",
        "\n",
        "    answer_ids = [token for token in generated[input_len:] if token != pad_id and token != 0]\n",
        "    answer = None\n",
        "\n",
        "    if answer_ids:\n",
        "        raw_answer = tokenizer.decode(answer_ids)\n",
        "        cleaned = clean_decode(answer_ids, tokenizer)\n",
        "\n",
        "        print(f\"🦪 Raw decoded: '{raw_answer}'\")\n",
        "        print(f\"🦹 Cleaned answer: '{cleaned}'\")\n",
        "\n",
        "        if not cleaned.strip():\n",
        "            answer = raw_answer.strip() or \"[empty]\"\n",
        "        elif cleaned.lower() in [\"null\", \"none\", \"pad\", \"<pad>\", \"<unk>\"]:\n",
        "            answer = raw_answer.strip()\n",
        "        else:\n",
        "            answer = cleaned\n",
        "    else:\n",
        "        answer = raw_answer.strip() if 'raw_answer' in locals() else \"[no output]\"\n",
        "\n",
        "    return answer\n",
        "\n",
        "def load_model(model_path=\"tinyqa_model.pth\", tokenizer_path=\"chatbot_bpe.model\", max_len=128,\n",
        "               d_model=64, num_heads=4, d_ff=128, num_layers=3, device=\"cpu\"):\n",
        "    with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "\n",
        "    vocab_size = tokenizer.sp.get_piece_size()\n",
        "    encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len=max_len)\n",
        "    model = TinyQAModel(encoder, d_model, vocab_size)\n",
        "\n",
        "    state_dict = torch.load(model_path, map_location=torch.device(device))\n",
        "    state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"encoder.pos_encoding.pe\")}\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🤖 TinyBot is ready! Type 'exit' to quit.\\n\")\n",
        "    test_examples = [\"Hi\", \"What's your name?\", \"Thanks\"]\n",
        "    print(\"Testing with examples:\")\n",
        "    for ex in test_examples:\n",
        "        answer = answer_question(model, tokenizer, ex, temperature=0.5)\n",
        "        print(f\"Q: {ex}\")\n",
        "        print(f\"A: {answer}\\n\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"You: \")\n",
        "        if q.lower() in ['exit', 'quit', 'bye']:\n",
        "            print(\"Bot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        answer = answer_question(model, tokenizer, q, temperature=0.5)\n",
        "        print(f\"Bot: {answer}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXGLsd_Y95En",
        "outputId": "ac604f2f-ed9f-4201-c0c4-8e41abab8d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import json\n",
        "import datetime\n",
        "from inference import answer_question,load_model\n",
        "from utils import BPETokenizer\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = load_model(\n",
        "    model_path=\"tinyqa_model.pth\",\n",
        "    tokenizer_path=\"chatbot_bpe.model\",\n",
        "    max_len=128,\n",
        "    d_model=64,\n",
        "    num_heads=4,\n",
        "    d_ff=128,\n",
        "    num_layers=3,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "# ------------------ Session State ------------------\n",
        "def init_session():\n",
        "    if \"history\" not in st.session_state:\n",
        "        st.session_state.history = []\n",
        "\n",
        "# ------------------ Sidebar ------------------\n",
        "def render_sidebar():\n",
        "    st.sidebar.title(\"🧠 Tiny Transformer\")\n",
        "    st.sidebar.markdown(\"Crafted by: **Sumanth Kadarla**\")\n",
        "    st.sidebar.markdown(\"🎓 B.Tech CSE (AIML), Tier-3\")\n",
        "    st.sidebar.markdown(\"📍 India | 🚀 FAANG-bound\")\n",
        "    st.sidebar.markdown(\"---\")\n",
        "    return st.sidebar.radio(\"📂 Navigate\", [\n",
        "        \"🏠 Home\",\n",
        "        \"💬 QA Chat\",\n",
        "        \"📜 History\",\n",
        "        \"📚 Dataset\",\n",
        "        \"📊 Training Monitor\",\n",
        "        \"⚙️ Model Settings\",\n",
        "        \"🧠 Architecture\",\n",
        "        \"📘 About\"\n",
        "    ])\n",
        "    st.sidebar.subheader(\"📂 Load Saved Model\")\n",
        "    model_file = st.sidebar.file_uploader(\"Upload Model (.pth)\", type=\"pth\")\n",
        "    tokenizer_file = st.sidebar.file_uploader(\"Upload Tokenizer (.model)\", type=\"model\")\n",
        "\n",
        "    if model_file and tokenizer_file:\n",
        "        with open(\"uploaded_model.pth\", \"wb\") as f:\n",
        "            f.write(model_file.read())\n",
        "        with open(\"uploaded_tokenizer.model\", \"wb\") as f:\n",
        "            f.write(tokenizer_file.read())\n",
        "\n",
        "        st.session_state.custom_model = \"uploaded_model.pth\"\n",
        "        st.session_state.custom_tokenizer = \"uploaded_tokenizer.model\"\n",
        "        st.sidebar.success(\"✅ Model & Tokenizer loaded!\")\n",
        "\n",
        "\n",
        "# ------------------ Home ------------------\n",
        "def render_home():\n",
        "    st.title(\"🏠 Welcome to TinyTransformer QA Playground\")\n",
        "    st.markdown(\"\"\"\n",
        "    A minimal transformer-based chatbot project to learn how GPT-like models work under the hood.\n",
        "\n",
        "    - Built from scratch using PyTorch\n",
        "    - Streamlit-powered dashboard\n",
        "    - Simple token-based Q&A transformer\n",
        "    \"\"\")\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"🧭 Navigate using the sidebar to explore features\")\n",
        "\n",
        "# ------------------ Chat ------------------\n",
        "def render_qa_chat():\n",
        "    st.title(\"💬 Ask a Question\")\n",
        "    tab1, tab2 = st.tabs([\"🧠 Ask\", \"🔍 Explanation (coming soon)\"])\n",
        "\n",
        "    with tab1:\n",
        "        user_input = st.text_input(\"Type your question:\")\n",
        "        if st.button(\"🎯 Get Answer\") and user_input:\n",
        "            prompt = user_input\n",
        "            answer = answer_question(model, tokenizer,prompt).strip()\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            st.session_state.history.append({\n",
        "                \"question\": user_input,\n",
        "                \"answer\": answer,\n",
        "                \"timestamp\": timestamp\n",
        "            })\n",
        "            st.success(f\"🤖 {answer}\")\n",
        "\n",
        "    with tab2:\n",
        "        st.info(\"Model explanation and token-wise breakdown will go here in a future update!\")\n",
        "\n",
        "\n",
        "# ------------------ History ------------------\n",
        "def render_history():\n",
        "    st.title(\"📜 Conversation History\")\n",
        "    if st.session_state.history:\n",
        "        for item in st.session_state.history[::-1]:\n",
        "            with st.expander(f\"🕒 {item['timestamp']}\"):\n",
        "                st.markdown(f\"**Q:** {item['question']}\")\n",
        "                st.markdown(f\"**A:** {item['answer']}\")\n",
        "    else:\n",
        "        st.info(\"No conversation history yet.\")\n",
        "\n",
        "    if st.button(\"💾 Save Chat\"):\n",
        "        with open(\"chat_history.json\", \"w\") as f:\n",
        "            json.dump(st.session_state.history, f)\n",
        "        st.success(\"Saved to chat_history.json\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"📂 Load Previous History\", type=\"json\")\n",
        "    if uploaded_file:\n",
        "        st.session_state.history.extend(json.load(uploaded_file))\n",
        "        st.success(\"History loaded.\")\n",
        "\n",
        "# ------------------ Dataset ------------------\n",
        "def render_dataset():\n",
        "    import pandas as pd\n",
        "\n",
        "    st.title(\"📚 Dataset Viewer\")\n",
        "    tab1, tab2 = st.tabs([\"🔍 Preview\", \"📥 Upload\"])\n",
        "\n",
        "    with tab1:\n",
        "        st.markdown(\"### 📄 Dataset Preview\")\n",
        "\n",
        "        if \"uploaded_df\" in st.session_state:\n",
        "            st.success(\"Showing uploaded dataset:\")\n",
        "            st.dataframe(st.session_state.uploaded_df)\n",
        "        else:\n",
        "            st.info(\"No dataset uploaded yet. Showing static preview instead.\")\n",
        "            st.table([\n",
        "                {\"Question\": \"What is AI?\", \"Answer\": \"Artificial Intelligence\"},\n",
        "                {\"Question\": \"Who is Elon Musk?\", \"Answer\": \"Entrepreneur\"},\n",
        "                {\"Question\": \"What is Python?\", \"Answer\": \"Programming\"},\n",
        "            ])\n",
        "\n",
        "    with tab2:\n",
        "        import chromadb\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "\n",
        "        st.header(\"📂 Upload & Embed Your Knowledge\")\n",
        "\n",
        "        if \"doc_chunks\" not in st.session_state:\n",
        "            st.session_state.doc_chunks = []\n",
        "\n",
        "        embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        chroma_client = chromadb.Client()\n",
        "        if \"rag_memory\" not in [c.name for c in chroma_client.list_collections()]:\n",
        "            collection = chroma_client.create_collection(\"rag_memory\")\n",
        "        else:\n",
        "            collection = chroma_client.get_collection(\"rag_memory\")\n",
        "\n",
        "        data_file = st.file_uploader(\"Upload a .txt file for knowledge base\", type=[\"txt\"])\n",
        "        if data_file:\n",
        "          try:\n",
        "\n",
        "              df = pd.read_csv(data_file, sep=\"\\t\")\n",
        "\n",
        "              # Basic validation\n",
        "              if \"Question\" in df.columns and \"Answer\" in df.columns:\n",
        "                  st.session_state.uploaded_df = df[[\"Question\", \"Answer\"]].dropna()\n",
        "                  st.success(f\"✅ Uploaded dataset with {len(st.session_state.uploaded_df)} valid QA pairs.\")\n",
        "              else:\n",
        "                  st.error(\"❌ The uploaded TSV must contain 'Question' and 'Answer' columns.\")\n",
        "          except Exception as e:\n",
        "              st.error(f\"❌ Failed to read TSV file: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------ Training Monitor ------------------\n",
        "def render_training():\n",
        "    import pandas as pd\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    import pickle\n",
        "    from torch.utils.data import DataLoader, TensorDataset\n",
        "    from utils import SimpleTokenizer, prepare_data\n",
        "    from model import TransformerEncoder, TinyQAModel\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    st.title(\"📊 Train Model Inside Streamlit\")\n",
        "\n",
        "    if \"uploaded_df\" not in st.session_state:\n",
        "        st.warning(\"Please upload a dataset first in the 📚 Dataset tab.\")\n",
        "        return\n",
        "\n",
        "    df = st.session_state.uploaded_df\n",
        "    df.columns = [col.lower() for col in df.columns]\n",
        "\n",
        "    # Detect columns\n",
        "    if \"question\" in df.columns and \"answer\" in df.columns:\n",
        "        q_col, a_col = \"question\", \"answer\"\n",
        "    elif \"text\" in df.columns and \"response\" in df.columns:\n",
        "        q_col, a_col = \"text\", \"response\"\n",
        "    else:\n",
        "        st.error(\"Dataset must contain either ['question', 'answer'] or ['text', 'response']\")\n",
        "        return\n",
        "\n",
        "    st.subheader(\"⚙️ Training Configuration\")\n",
        "    epochs = st.slider(\"Epochs\", 1, 20, 5)\n",
        "    batch_size = st.selectbox(\"Batch Size\", [2, 4, 8, 16], index=1)\n",
        "    lr = st.number_input(\"Learning Rate\", value=1e-3, format=\"%.5f\")\n",
        "    max_len = st.slider(\"Max Token Length\", 10, 100, 50)\n",
        "    st.subheader(\"⚙️ Transformer Hyperparameters\")\n",
        "\n",
        "# 🧩 These are OUTSIDE the if-condition\n",
        "    d_model = st.slider(\"Model Size (d_model)\", 16, 128, 32, step=16)\n",
        "    num_heads = st.slider(\"Number of Attention Heads\", 1, 8, 2)\n",
        "    num_layers = st.slider(\"Number of Encoder Layers\", 1, 6, 2)\n",
        "    d_ff = st.slider(\"Feedforward Hidden Dim (d_ff)\", 32, 256, 64, step=32)\n",
        "    max_len = st.slider(\"Max Sequence Length\", 10, 100, 50)\n",
        "\n",
        "    # Optional model summary live preview\n",
        "    st.code(f\"\"\"\n",
        "    TinyQAModel(\n",
        "      encoder = TransformerEncoder(\n",
        "        layers = {num_layers},\n",
        "        heads = {num_heads},\n",
        "        d_model = {d_model},\n",
        "        d_ff = {d_ff},\n",
        "        max_len = {max_len}\n",
        "      ),\n",
        "      decoder = nn.Linear({d_model} → vocab)\n",
        "    )\n",
        "    \"\"\", language=\"python\")\n",
        "\n",
        "    if st.button(\"🚀 Train Model\"):\n",
        "        df = df[df[q_col].notna() & df[a_col].notna()]\n",
        "        df = df.astype({q_col: str, a_col: str})\n",
        "        uploaded_data = list(zip(df[q_col], df[a_col]))\n",
        "\n",
        "        chat_data = [\n",
        "            (\"Hi\", \"Hello!\"),\n",
        "            (\"Hey there\", \"Hi!\"),\n",
        "            (\"How are you?\", \"I'm doing well, thanks!\"),\n",
        "            (\"What's your name?\", \"I'm your chatbot.\"),\n",
        "            (\"What can you do?\", \"I answer questions.\"),\n",
        "            (\"Who made you?\", \"A student learning AI.\"),\n",
        "            (\"Tell me a joke\", \"Why did the computer get cold? It left its Windows open!\"),\n",
        "            (\"Nice joke\", \"Glad you liked it!\"),\n",
        "            (\"Do you like pizza?\", \"I love data... but pizza sounds good too.\"),\n",
        "            (\"What is AI?\", \"Artificial Intelligence.\"),\n",
        "            (\"What is ML?\", \"Machine Learning.\"),\n",
        "            (\"Tell me something cool\", \"Transformers power modern AI like ChatGPT!\"),\n",
        "            (\"Can you help me?\", \"Of course. What do you need?\"),\n",
        "            (\"Are you alive?\", \"Not yet 😄\"),\n",
        "            (\"Bye\", \"Goodbye!\"),\n",
        "            (\"Thanks\", \"You're welcome!\"),\n",
        "            (\"What’s 2 + 2?\", \"It’s 4.\"),\n",
        "            (\"What's the capital of India?\", \"New Delhi.\"),\n",
        "            (\"Can you sing?\", \"I can rhyme in binary.\"),\n",
        "            (\"What is Python?\", \"A programming language.\")\n",
        "        ]\n",
        "\n",
        "\n",
        "        # Combine base and uploaded dataset\n",
        "        combined_data = chat_data + uploaded_data\n",
        "\n",
        "        # ⛔ Clean\n",
        "        combined_data = [(q, a) for q, a in combined_data if isinstance(q, str) and isinstance(a, str)]\n",
        "\n",
        "        # ✅ Split\n",
        "        train_data, val_data = train_test_split(combined_data, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Fit tokenizer on **all** data\n",
        "        tokenizer = BPETokenizer(\"chatbot_bpe.model\")\n",
        "        # Prepare tensors\n",
        "        X_train, y_train = prepare_data(train_data, tokenizer, max_len)\n",
        "        X_val, y_val = prepare_data(val_data, tokenizer, max_len)\n",
        "\n",
        "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
        "\n",
        "        vocab_size = tokenizer.sp.get_piece_size()\n",
        "\n",
        "        encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len=max_len)\n",
        "        model = TinyQAModel(encoder, d_model, vocab_size)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        loss_history = []\n",
        "\n",
        "        progress_bar = st.progress(0)\n",
        "        status_text = st.empty()\n",
        "        train_loss_history = []\n",
        "        val_loss_history = []\n",
        "        val_acc_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "          model.train()\n",
        "          total_loss = 0\n",
        "          for batch_x, batch_y in train_loader:\n",
        "              logits = model(batch_x)\n",
        "              logits = logits.view(-1, vocab_size)\n",
        "              targets = batch_y.view(-1)\n",
        "\n",
        "              loss = criterion(logits, targets)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              optimizer.zero_grad()\n",
        "              total_loss += loss.item()\n",
        "\n",
        "          avg_train_loss = total_loss / len(train_loader)\n",
        "          train_loss_history.append(avg_train_loss)\n",
        "\n",
        "          # 🔍 VALIDATION\n",
        "          model.eval()\n",
        "          val_loss = 0\n",
        "          correct = 0\n",
        "          total = 0\n",
        "          with torch.no_grad():\n",
        "              for val_x, val_y in val_loader:\n",
        "                  logits = model(val_x)\n",
        "                  logits = logits.view(-1, vocab_size)\n",
        "                  targets = val_y.view(-1)\n",
        "                  loss = criterion(logits, targets)\n",
        "                  val_loss += loss.item()\n",
        "\n",
        "                  preds = torch.argmax(logits, dim=1)\n",
        "                  correct += (preds == targets).sum().item()\n",
        "                  total += targets.size(0)\n",
        "\n",
        "          avg_val_loss = val_loss / len(val_loader)\n",
        "          val_acc = correct / total\n",
        "\n",
        "          val_loss_history.append(avg_val_loss)\n",
        "          val_acc_history.append(val_acc)\n",
        "\n",
        "          progress_bar.progress((epoch + 1) / epochs)\n",
        "          status_text.text(\n",
        "              f\"Epoch {epoch + 1}/{epochs} | \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f} | \"\n",
        "              f\"Val Acc: {val_acc:.2%}\"\n",
        "          )\n",
        "        torch.save(model.state_dict(), \"tinyqa_model.pth\")\n",
        "        with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "            pickle.dump(tokenizer, f)\n",
        "        misclassified = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for val_x, val_y in val_loader:\n",
        "                logits = model(val_x)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                for x, y_true, y_pred in zip(val_x, val_y, preds):\n",
        "                    if y_true != y_pred:\n",
        "                        question = tokenizer.decode([t for t in x.tolist() if t != 0])\n",
        "                        answer = tokenizer.decode([y_true.item()])\n",
        "                        predicted = tokenizer.decode([y_pred.item()])\n",
        "                        misclassified.append((question, answer, predicted))\n",
        "\n",
        "        if misclassified:\n",
        "            st.subheader(\"❌ Misclassified Examples\")\n",
        "            for q, true_a, pred_a in misclassified[:5]:  # limit to 5 for speed\n",
        "                st.markdown(f\"**Q:** {q}\")\n",
        "                st.markdown(f\"**True A:** {true_a} | **Pred A:** {pred_a}\")\n",
        "                st.markdown(\"---\")\n",
        "\n",
        "        st.success(\"✅ Model trained and saved!\")\n",
        "        st.subheader(\"📉 Training Progress\")\n",
        "\n",
        "        chart_data = {\n",
        "            \"Train Loss\": train_loss_history,\n",
        "            \"Val Loss\": val_loss_history,\n",
        "            \"Val Accuracy\": val_acc_history\n",
        "        }\n",
        "        st.line_chart(chart_data)\n",
        "        model_name = st.text_input(\"Model name to save (no extension)\", \"tinyqa_bpe\")\n",
        "        if st.button(\"💾 Save Model\"):\n",
        "            torch.save(model.state_dict(), f\"{model_name}.pth\")\n",
        "            with open(f\"{model_name}_tokenizer.model\", \"wb\") as f:\n",
        "                f.write(open(\"chatbot_bpe.model\", \"rb\").read())\n",
        "            st.success(f\"Model and tokenizer saved as {model_name}.pth and {model_name}_tokenizer.model\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------ Settings ------------------\n",
        "def render_settings():\n",
        "    st.title(\"⚙️ Model Settings\")\n",
        "    model_path = st.session_state.get(\"custom_model\", \"tinyqa_bpe_model.pth\")\n",
        "    tokenizer_path = st.session_state.get(\"custom_tokenizer\", \"chatbot_bpe.model\")\n",
        "\n",
        "\n",
        "    st.info(f\"\"\"\n",
        "    **Current Model**: `{model_path}`\n",
        "    **Tokenizer**: `{tokenizer_path}`\n",
        "    \"\"\")\n",
        "\n",
        "    if st.button(\"🔁 Reset to Default\"):\n",
        "        st.session_state.custom_model = \"tinyqa_bpe_model.pth\"\n",
        "        st.session_state.custom_tokenizer = \"chatbot_bpe.model\"\n",
        "        st.success(\"Reset to default model.\")\n",
        "# ------------------ Architecture ------------------\n",
        "def render_architecture():\n",
        "    st.title(\"🧠 Model Architecture\")\n",
        "    st.code(\"\"\"\n",
        "TinyQAModel(\n",
        "  encoder = TransformerEncoder(...),\n",
        "  decoder = nn.Linear(d_model → vocab)\n",
        ")\n",
        "    \"\"\", language=\"python\")\n",
        "    st.markdown(\"Heads: 2 | Layers: 2 | d_model: 32 | max_len: 5\")\n",
        "\n",
        "# ------------------ About ------------------\n",
        "def render_about():\n",
        "    st.title(\"📘 About This App\")\n",
        "    st.markdown(\"\"\"\n",
        "This app is a **mini Transformer QA bot** built from scratch using PyTorch.\n",
        "\n",
        "**Built by:** Sumanth\n",
        "**Degree:** B.Tech CSE (AIML), Tier-3\n",
        "**Current Focus:** ML Engineering | Strategic AI Roles\n",
        "    \"\"\")\n",
        "    st.info(\"Built for learning and showcasing LLM mechanics. Updates coming soon!\")\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"💡 [GitHub Repo](#) | 🧠 Powered by PyTorch + Streamlit + SentencePiece\")\n",
        "\n",
        "# ------------------ Main ------------------\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"TinyQA Bot\", layout=\"wide\", initial_sidebar_state=\"collapsed\")\n",
        "    init_session()\n",
        "    section = render_sidebar()\n",
        "\n",
        "    if section == \"🏠 Home\":\n",
        "        render_home()\n",
        "    elif section == \"💬 QA Chat\":\n",
        "        render_qa_chat()\n",
        "    elif section == \"📜 History\":\n",
        "        render_history()\n",
        "    elif section == \"📚 Dataset\":\n",
        "        render_dataset()\n",
        "    elif section == \"📊 Training Monitor\":\n",
        "        render_training()\n",
        "    elif section == \"⚙️ Model Settings\":\n",
        "        render_settings()\n",
        "    elif section == \"🧠 Architecture\":\n",
        "        render_architecture()\n",
        "    elif section == \"📘 About\":\n",
        "        render_about()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "illz1OQT95CI",
        "outputId": "ff3001ae-5062-4829-a467-365a3022e81d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "OKSOwMQL944u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03bd213f-731d-49a2-95fe-b438ea2c5764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting diagnostic_script.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile diagnostic_script.py\n",
        "import torch\n",
        "import pickle\n",
        "from model import TransformerEncoder, TinyQAModel\n",
        "from utils import BPETokenizer\n",
        "\n",
        "print(\"🔍 Running diagnostics...\")\n",
        "\n",
        "# Load tokenizer\n",
        "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "print(f\"✓ Tokenizer loaded\")\n",
        "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"  EOS token ID: {tokenizer.sp.eos_id()}\")\n",
        "print(f\"  PAD token ID: {tokenizer.vocab_size}\")\n",
        "\n",
        "# Test tokenizer\n",
        "test_text = \"Hello world\"\n",
        "encoded = tokenizer.encode(test_text)\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(f\"\\n✓ Tokenizer test:\")\n",
        "print(f\"  Original: '{test_text}'\")\n",
        "print(f\"  Encoded: {encoded}\")\n",
        "print(f\"  Decoded: '{decoded}'\")\n",
        "\n",
        "# Load model\n",
        "max_len = 128\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "d_ff = 128\n",
        "pad_id=800\n",
        "num_layers = 2\n",
        "vocab_size = tokenizer.sp.get_piece_size()\n",
        "\n",
        "encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len=max_len)\n",
        "model = TinyQAModel(encoder, d_model, vocab_size)\n",
        "\n",
        "print(f\"\\n✓ Model created\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Load weights\n",
        "try:\n",
        "    state_dict = torch.load(\"tinyqa_model.pth\", map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(state_dict,strict=False)\n",
        "    model.eval()\n",
        "    print(f\"✓ Model weights loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading model weights: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Test forward pass\n",
        "print(f\"\\n🧪 Testing model forward pass...\")\n",
        "test_input = \"Hi\"\n",
        "pad_id = tokenizer.sp.get_piece_size() - 1\n",
        "input_ids = tokenizer.encode(test_input)[:25] + [tokenizer.sp.eos_id()]\n",
        "padded = input_ids + [pad_id] * (max_len - len(input_ids))\n",
        "input_tensor = torch.tensor([padded], dtype=torch.long)\n",
        "\n",
        "print(f\"  Input text: '{test_input}'\")\n",
        "print(f\"  Input IDs: {input_ids}\")\n",
        "print(f\"  Input shape: {input_tensor.shape}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    logits = output\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    decoded_input = predicted_ids[0].tolist() if predicted_ids.ndim == 2 else predicted_ids.tolist()\n",
        "    response = tokenizer.decode(decoded_input)\n",
        "\n",
        "    print(f\"  Output shape: {output.shape}\")\n",
        "\n",
        "    # Check predictions at each position\n",
        "    print(f\"\\n  Token predictions:\")\n",
        "    for i in range(len(input_ids)):\n",
        "        token_logits = output[0, i, :]\n",
        "        top_tokens = torch.topk(token_logits, 5)\n",
        "        top_ids = top_tokens.indices.tolist()\n",
        "        top_probs = torch.softmax(top_tokens.values, dim=-1).tolist()\n",
        "\n",
        "        print(f\"    Position {i}: {tokenizer.decode([padded[i]])} ->\")\n",
        "        for tid, prob in zip(top_ids, top_probs):\n",
        "            decoded = tokenizer.decode([tid])\n",
        "            print(f\"      {decoded}: {prob:.3f}\")\n",
        "\n",
        "# Test generation\n",
        "print(f\"\\n🧪 Testing generation...\")\n",
        "\n",
        "def debug_generate(question):\n",
        "    q_ids = tokenizer.encode(question)[:25]\n",
        "    input_ids = q_ids + [tokenizer.sp.eos_id()]\n",
        "\n",
        "    print(f\"  Question: '{question}'\")\n",
        "    print(f\"  Question IDs: {q_ids}\")\n",
        "\n",
        "    generated = []\n",
        "    with torch.no_grad():\n",
        "        for step in range(10):  # Generate up to 10 tokens\n",
        "            current_seq = input_ids + generated\n",
        "            padded = current_seq + [tokenizer.sp.pad_id()] * (max_len - len(current_seq))\n",
        "            input_tensor = torch.tensor([padded], dtype=torch.long)\n",
        "\n",
        "            output = model(input_tensor)\n",
        "            next_pos = len(current_seq) - 1\n",
        "\n",
        "            if next_pos >= max_len - 1:\n",
        "                break\n",
        "\n",
        "            logits = output[0, next_pos, :]\n",
        "            next_token = torch.argmax(logits).item()\n",
        "\n",
        "            print(f\"    Step {step}: pos={next_pos}, next_token={next_token} ('{tokenizer.decode([next_token])}')\")\n",
        "\n",
        "            if next_token == tokenizer.sp.eos_id() or next_token == tokenizer.sp.pad_id():\n",
        "                print(f\"    Stopping: hit EOS/PAD\")\n",
        "                break\n",
        "\n",
        "            generated.append(next_token)\n",
        "\n",
        "    if generated:\n",
        "        answer = tokenizer.decode(generated)\n",
        "        print(f\"  Generated tokens: {generated}\")\n",
        "        print(f\"  Answer: '{answer}'\")\n",
        "    else:\n",
        "        print(f\"  No tokens generated!\")\n",
        "\n",
        "# Test with different questions\n",
        "for q in [\"Hi\", \"Hello\", \"What's your name?\"]:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    debug_generate(q)\n",
        "from inference import answer_question\n",
        "\n",
        "print(\"\\n🚨 Testing live inference:\")\n",
        "qs = [\"Hi\", \"Hello\", \"What's your name?\"]\n",
        "for q in qs:\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {answer_question(model,tokenizer,q)}\\n\")\n",
        "\n",
        "print(\"\\n✅ Diagnostics complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "tuvEMa9u942Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e95bdd7-eb05-4494-8b9a-0843414f6c77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-02 14:19:18.565658: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754144358.590390   86646 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754144358.597695   86646 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "🔍 Running diagnostics...\n",
            "✓ Tokenizer loaded\n",
            "  Vocab size: 800\n",
            "  EOS token ID: 2\n",
            "  PAD token ID: 800\n",
            "\n",
            "✓ Tokenizer test:\n",
            "  Original: 'Hello world'\n",
            "  Encoded: [335, 30, 283]\n",
            "  Decoded: 'Hello world'\n",
            "\n",
            "✓ Model created\n",
            "  Total parameters: 170,337\n",
            "✓ Model weights loaded successfully\n",
            "\n",
            "🧪 Testing model forward pass...\n",
            "  Input text: 'Hi'\n",
            "  Input IDs: [251, 2]\n",
            "  Input shape: torch.Size([1, 128])\n",
            "  Output shape: torch.Size([1, 128, 801])\n",
            "\n",
            "  Token predictions:\n",
            "    Position 0: Hi ->\n",
            "      Hello: 0.744\n",
            "      H: 0.070\n",
            "      Qu: 0.067\n",
            "      T: 0.063\n",
            "      As: 0.056\n",
            "    Position 1:  ->\n",
            "      there: 0.254\n",
            "      ning: 0.215\n",
            "      !: 0.199\n",
            "      atile: 0.176\n",
            "      that: 0.155\n",
            "\n",
            "🧪 Testing generation...\n",
            "\n",
            "==================================================\n",
            "  Question: 'Hi'\n",
            "  Question IDs: [251]\n",
            "    Step 0: pos=1, next_token=769 ('!')\n",
            "    Step 1: pos=2, next_token=765 (''')\n",
            "    Step 2: pos=3, next_token=765 (''')\n",
            "    Step 3: pos=4, next_token=765 (''')\n",
            "    Step 4: pos=5, next_token=765 (''')\n",
            "    Step 5: pos=6, next_token=765 (''')\n",
            "    Step 6: pos=7, next_token=765 (''')\n",
            "    Step 7: pos=8, next_token=765 (''')\n",
            "    Step 8: pos=9, next_token=765 (''')\n",
            "    Step 9: pos=10, next_token=765 (''')\n",
            "  Generated tokens: [769, 765, 765, 765, 765, 765, 765, 765, 765, 765]\n",
            "  Answer: '!''''''''''\n",
            "\n",
            "==================================================\n",
            "  Question: 'Hello'\n",
            "  Question IDs: [335]\n",
            "    Step 0: pos=1, next_token=344 ('there')\n",
            "    Step 1: pos=2, next_token=12 ('is')\n",
            "    Step 2: pos=3, next_token=12 ('is')\n",
            "    Step 3: pos=4, next_token=12 ('is')\n",
            "    Step 4: pos=5, next_token=500 ('ause')\n",
            "    Step 5: pos=6, next_token=765 (''')\n",
            "    Step 6: pos=7, next_token=765 (''')\n",
            "    Step 7: pos=8, next_token=765 (''')\n",
            "    Step 8: pos=9, next_token=765 (''')\n",
            "    Step 9: pos=10, next_token=765 (''')\n",
            "  Generated tokens: [344, 12, 12, 12, 500, 765, 765, 765, 765, 765]\n",
            "  Answer: 'there is is isause''''''\n",
            "\n",
            "==================================================\n",
            "  Question: 'What's your name?'\n",
            "  Question IDs: [21, 765, 740, 332, 624, 756]\n",
            "    Step 0: pos=6, next_token=332 ('your')\n",
            "    Step 1: pos=7, next_token=45 ('T')\n",
            "    Step 2: pos=8, next_token=543 ('You')\n",
            "    Step 3: pos=9, next_token=647 ('istant')\n",
            "    Step 4: pos=10, next_token=754 ('.')\n",
            "    Step 5: pos=11, next_token=765 (''')\n",
            "    Step 6: pos=12, next_token=765 (''')\n",
            "    Step 7: pos=13, next_token=765 (''')\n",
            "    Step 8: pos=14, next_token=765 (''')\n",
            "    Step 9: pos=15, next_token=765 (''')\n",
            "  Generated tokens: [332, 45, 543, 647, 754, 765, 765, 765, 765, 765]\n",
            "  Answer: 'your T Youistant.''''''\n",
            "\n",
            "🚨 Testing live inference:\n",
            "Q: Hi\n",
            "🔎 Step 0: token 152 → 'and'\n",
            "🔎 Step 1: token 151 → 'The'\n",
            "🔎 Step 2: token 192 → 'Pyt'\n",
            "🔎 Step 3: token 627 → 'reus'\n",
            "🔎 Step 4: token 599 → 'Good'\n",
            "🔎 Step 5: token 715 → 'humans'\n",
            "🔎 Step 6: token 37 → 'A'\n",
            "🔎 Step 7: token 227 → 'ans'\n",
            "🔎 Step 8: token 749 → 'm'\n",
            "🔎 Step 9: token 661 → 'build'\n",
            "🔎 Step 10: token 84 → 'g'\n",
            "🔎 Step 11: token 769 → '!'\n",
            "🔎 Step 12: token 724 → 'nerves'\n",
            "🔎 Step 13: token 603 → 'Take'\n",
            "🔎 Step 14: token 12 → 'is'\n",
            "🔎 Step 15: token 373 → 'smartphone'\n",
            "🔎 Step 16: token 622 → 'mole'\n",
            "🔎 Step 17: token 706 → 'during'\n",
            "🔎 Step 18: token 297 → 'con'\n",
            "🔎 Step 19: token 96 → 'ct'\n",
            "🔎 Step 20: token 473 → 'As'\n",
            "🔎 Step 21: token 359 → 'process'\n",
            "🔎 Step 22: token 117 → 'was'\n",
            "🔎 Step 23: token 23 → 'le'\n",
            "🔎 Step 24: token 668 → 'featu'\n",
            "🔎 Step 25: token 281 → 'loud'\n",
            "🔎 Step 26: token 648 → 'vanced'\n",
            "🔎 Step 27: token 556 → 'has'\n",
            "🔎 Step 28: token 356 → 'capital'\n",
            "🔎 Step 29: token 149 → 'ence'\n",
            "🔎 Step 30: token 515 → 'iden'\n",
            "🔎 Step 31: token 784 → 'U'\n",
            "🔎 Step 32: token 61 → 'ut'\n",
            "🔎 Step 33: token 232 → 'eed'\n",
            "🔎 Step 34: token 578 → 'erson'\n",
            "🔎 Step 35: token 280 → 'king'\n",
            "🔎 Step 36: token 47 → 'st'\n",
            "🔎 Step 37: token 515 → 'iden'\n",
            "🔎 Step 38: token 349 → 'Python'\n",
            "🔎 Step 39: token 673 → 'learn'\n",
            "🔎 Step 40: token 325 → 'make'\n",
            "🔎 Step 41: token 329 → 'used'\n",
            "🔎 Step 42: token 494 → 'ants'\n",
            "🔎 Step 43: token 602 → 'Sure'\n",
            "🔎 Step 44: token 84 → 'g'\n",
            "🔎 Step 45: token 275 → 'ific'\n",
            "🔎 Step 46: token 227 → 'ans'\n",
            "🔎 Step 47: token 603 → 'Take'\n",
            "🔎 Step 48: token 19 → 'ing'\n",
            "🔎 Step 49: token 204 → 'smart'\n",
            "🧪 Raw decoded: 'and The Pyt reus Good humans Aansm build g! nerves Take is smartphone mole during conct As process wasle featuloudvanced has capitalenceidenUuteedersonking stiden Python learn make usedants Sure gificans Takeing smart'\n",
            "🧹 Cleaned answer: 'and The Pyt reus Good humans Aansm build g! nerves Take is smartphone mole during conct As process wasle featuloudvanced has capitalenceidenUuteedersonking stiden Python learn make usedants Sure gificans Takeing smart'\n",
            "A: and The Pyt reus Good humans Aansm build g! nerves Take is smartphone mole during conct As process wasle featuloudvanced has capitalenceidenUuteedersonking stiden Python learn make usedants Sure gificans Takeing smart\n",
            "\n",
            "Q: Hello\n",
            "🔎 Step 0: token 251 → 'Hi'\n",
            "🔎 Step 1: token 335 → 'Hello'\n",
            "🔎 Step 2: token 75 → 'ning'\n",
            "🔎 Step 3: token 595 → '1939'\n",
            "🔎 Step 4: token 207 → 'state'\n",
            "🔎 Step 5: token 54 → 'to'\n",
            "🔎 Step 6: token 489 → 'we'\n",
            "🔎 Step 7: token 522 → 'leep'\n",
            "🔎 Step 8: token 219 → 'ue'\n",
            "🔎 Step 9: token 251 → 'Hi'\n",
            "🔎 Step 10: token 725 → 'person'\n",
            "🔎 Step 11: token 280 → 'king'\n",
            "🔎 Step 12: token 771 → 'P'\n",
            "🔎 Step 13: token 630 → 'spec'\n",
            "🔎 Step 14: token 455 → 'ory'\n",
            "🔎 Step 15: token 660 → 'block'\n",
            "🔎 Step 16: token 371 → 'ransformers'\n",
            "🔎 Step 17: token 124 → 'ec'\n",
            "🔎 Step 18: token 217 → 'lp'\n",
            "🔎 Step 19: token 326 → 'math'\n",
            "🔎 Step 20: token 352 → 'living'\n",
            "🔎 Step 21: token 290 → 'Ein'\n",
            "🔎 Step 22: token 333 → 'apital'\n",
            "🔎 Step 23: token 0 → ''\n",
            "🛑 Decoding stopped (EOS or PAD)\n",
            "🧪 Raw decoded: 'Hi Helloning 1939 state to weleepue Hi personkingP specory blockransformerseclp math living Einapital'\n",
            "🧹 Cleaned answer: 'Hi Helloning 1939 state to weleepue Hi personkingP specory blockransformerseclp math living Einapital'\n",
            "A: Hi Helloning 1939 state to weleepue Hi personkingP specory blockransformerseclp math living Einapital\n",
            "\n",
            "Q: What's your name?\n",
            "🔎 Step 0: token 364 → 'networks'\n",
            "🔎 Step 1: token 620 → 'mass'\n",
            "🔎 Step 2: token 713 → 'global'\n",
            "🔎 Step 3: token 351 → 'access'\n",
            "🔎 Step 4: token 734 → 'strong'\n",
            "🔎 Step 5: token 377 → '2?'\n",
            "🔎 Step 6: token 344 → 'there'\n",
            "🔎 Step 7: token 12 → 'is'\n",
            "🔎 Step 8: token 373 → 'smartphone'\n",
            "🔎 Step 9: token 765 → '''\n",
            "🔎 Step 10: token 765 → '''\n",
            "🔎 Step 11: token 703 → 'branch'\n",
            "🔎 Step 12: token 641 → 'atters'\n",
            "🔎 Step 13: token 740 → 's'\n",
            "🔎 Step 14: token 555 → 'han'\n",
            "🔎 Step 15: token 114 → 'for'\n",
            "🔎 Step 16: token 673 → 'learn'\n",
            "🔎 Step 17: token 652 → 'Cells'\n",
            "🔎 Step 18: token 356 → 'capital'\n",
            "🔎 Step 19: token 24 → 'of'\n",
            "🔎 Step 20: token 776 → 'C'\n",
            "🔎 Step 21: token 302 → 'not'\n",
            "🔎 Step 22: token 600 → 'Mach'\n",
            "🔎 Step 23: token 122 → 'am'\n",
            "🔎 Step 24: token 672 → 'langu'\n",
            "🔎 Step 25: token 477 → 'Qu'\n",
            "🔎 Step 26: token 244 → 'lou'\n",
            "🔎 Step 27: token 361 → 'quantum'\n",
            "🔎 Step 28: token 39 → 'st'\n",
            "🔎 Step 29: token 692 → 'cluding'\n",
            "🔎 Step 30: token 790 → '’'\n",
            "🔎 Step 31: token 201 → 'than'\n",
            "🔎 Step 32: token 639 → 'angled'\n",
            "🔎 Step 33: token 370 → 'statement'\n",
            "🔎 Step 34: token 477 → 'Qu'\n",
            "🔎 Step 35: token 641 → 'atters'\n",
            "🔎 Step 36: token 33 → 'd'\n",
            "🔎 Step 37: token 723 → 'mobile'\n",
            "🔎 Step 38: token 53 → 'ion'\n",
            "🔎 Step 39: token 7 → 'a'\n",
            "🔎 Step 40: token 401 → 'rt'\n",
            "🔎 Step 41: token 37 → 'A'\n",
            "🔎 Step 42: token 758 → 'b'\n",
            "🔎 Step 43: token 293 → 'Why'\n",
            "🔎 Step 44: token 247 → 'ors'\n",
            "🔎 Step 45: token 547 → 'bur'\n",
            "🔎 Step 46: token 195 → 'antum'\n",
            "🔎 Step 47: token 39 → 'st'\n",
            "🔎 Step 48: token 646 → 'inyBot'\n",
            "🔎 Step 49: token 37 → 'A'\n",
            "🧪 Raw decoded: 'networks mass global access strong2? there is smartphone'' branchatterss han for learn Cells capital ofC not Macham langu Qulou quantumstcluding’ thanangled statement Quatters d mobileion art Ab Whyors burantumstinyBot A'\n",
            "🧹 Cleaned answer: 'networks mass global access strong2? there is smartphone'' branchatterss han for learn Cells capital ofC not Macham langu Qulou quantumstcluding’ thanangled statement Quatters d mobileion art Ab Whyors burantumstinyBot A'\n",
            "A: networks mass global access strong2? there is smartphone'' branchatterss han for learn Cells capital ofC not Macham langu Qulou quantumstcluding’ thanangled statement Quatters d mobileion art Ab Whyors burantumstinyBot A\n",
            "\n",
            "\n",
            "✅ Diagnostics complete!\n"
          ]
        }
      ],
      "source": [
        "!python diagnostic_script.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "VNQ8cEbB94xP"
      },
      "outputs": [],
      "source": [
        "!rm chatbot_bpe.*  # Delete old model if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5NgUVyT94mB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGlVjo23L-dB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcaFEhsbSx2vlhpMpdgOg0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}